<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="21 NLP基础（上）：详解自然语言处理原理与常用算法" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>21 NLP基础（上）：详解自然语言处理原理与常用算法 </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%e5%a6%82%e4%bd%95%e9%ab%98%e6%95%88%e5%85%a5%e9%97%a8PyTorch%ef%bc%9f.md.html" id="00 开篇词 如何高效入门PyTorch？.md.html">00 开篇词 如何高效入门PyTorch？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/01%20PyTorch%ef%bc%9a%e7%bd%91%e7%ba%a2%e4%b8%ad%e7%9a%84%e9%a1%b6%e6%b5%81%e6%98%8e%e6%98%9f.md.html" id="01 PyTorch：网红中的顶流明星.md.html">01 PyTorch：网红中的顶流明星.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/02%20NumPy%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e6%a0%b8%e5%bf%83%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84%e8%af%a6%e8%a7%a3.md.html" id="02 NumPy（上）：核心数据结构详解.md.html">02 NumPy（上）：核心数据结构详解.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/03%20NumPy%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%b8%b8%e7%94%a8%e6%93%8d%e4%bd%9c.md.html" id="03 NumPy（下）：深度学习中的常用操作.md.html">03 NumPy（下）：深度学习中的常用操作.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/04%20Tensor%ef%bc%9aPyTorch%e4%b8%ad%e6%9c%80%e5%9f%ba%e7%a1%80%e7%9a%84%e8%ae%a1%e7%ae%97%e5%8d%95%e5%85%83.md.html" id="04 Tensor：PyTorch中最基础的计算单元.md.html">04 Tensor：PyTorch中最基础的计算单元.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/05%20Tensor%e5%8f%98%e5%bd%a2%e8%ae%b0%ef%bc%9a%e5%bf%ab%e9%80%9f%e6%8e%8c%e6%8f%a1Tensor%e5%88%87%e5%88%86%e3%80%81%e5%8f%98%e5%bd%a2%e7%ad%89%e6%96%b9%e6%b3%95.md.html" id="05 Tensor变形记：快速掌握Tensor切分、变形等方法.md.html">05 Tensor变形记：快速掌握Tensor切分、变形等方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/06%20Torchvision%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e6%95%b0%e6%8d%ae%e8%af%bb%e5%8f%96%ef%bc%8c%e8%ae%ad%e7%bb%83%e5%bc%80%e5%a7%8b%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5.md.html" id="06 Torchvision（上）：数据读取，训练开始的第一步.md.html">06 Torchvision（上）：数据读取，训练开始的第一步.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/07%20Torchvision%ef%bc%88%e4%b8%ad%ef%bc%89%ef%bc%9a%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%ef%bc%8c%e8%ae%a9%e6%95%b0%e6%8d%ae%e6%9b%b4%e5%8a%a0%e5%a4%9a%e6%a0%b7%e6%80%a7.md.html" id="07 Torchvision（中）：数据增强，让数据更加多样性.md.html">07 Torchvision（中）：数据增强，让数据更加多样性.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/08%20Torchvision%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e5%85%b6%e4%bb%96%e6%9c%89%e8%b6%a3%e7%9a%84%e5%8a%9f%e8%83%bd.md.html" id="08 Torchvision（下）：其他有趣的功能.md.html">08 Torchvision（下）：其他有趣的功能.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/09%20%e5%8d%b7%e7%a7%af%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e5%a6%82%e4%bd%95%e7%94%a8%e5%8d%b7%e7%a7%af%e4%b8%ba%e8%ae%a1%e7%ae%97%e6%9c%ba%e2%80%9c%e5%bc%80%e5%a4%a9%e7%9c%bc%e2%80%9d%ef%bc%9f.md.html" id="09 卷积（上）：如何用卷积为计算机“开天眼”？.md.html">09 卷积（上）：如何用卷积为计算机“开天眼”？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/10%20%e5%8d%b7%e7%a7%af%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e5%a6%82%e4%bd%95%e7%94%a8%e5%8d%b7%e7%a7%af%e4%b8%ba%e8%ae%a1%e7%ae%97%e6%9c%ba%e2%80%9c%e5%bc%80%e5%a4%a9%e7%9c%bc%e2%80%9d%ef%bc%9f.md.html" id="10 卷积（下）：如何用卷积为计算机“开天眼”？.md.html">10 卷积（下）：如何用卷积为计算机“开天眼”？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/11%20%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%ef%bc%9a%e5%a6%82%e4%bd%95%e5%b8%ae%e5%8a%a9%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%bc%9a%e2%80%9c%e8%87%aa%e7%9c%81%e2%80%9d%ef%bc%9f.md.html" id="11 损失函数：如何帮助模型学会“自省”？.md.html">11 损失函数：如何帮助模型学会“自省”？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/12%20%e8%ae%a1%e7%ae%97%e6%a2%af%e5%ba%a6%ef%bc%9a%e7%bd%91%e7%bb%9c%e7%9a%84%e5%89%8d%e5%90%91%e4%b8%8e%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad.md.html" id="12 计算梯度：网络的前向与反向传播.md.html">12 计算梯度：网络的前向与反向传播.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/13%20%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95%ef%bc%9a%e6%9b%b4%e6%96%b0%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e7%9a%84%e6%96%b9%e6%b3%95.md.html" id="13 优化方法：更新模型参数的方法.md.html">13 优化方法：更新模型参数的方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/14%20%e6%9e%84%e5%bb%ba%e7%bd%91%e7%bb%9c%ef%bc%9a%e4%b8%80%e7%ab%99%e5%bc%8f%e5%ae%9e%e7%8e%b0%e6%a8%a1%e5%9e%8b%e6%90%ad%e5%bb%ba%e4%b8%8e%e8%ae%ad%e7%bb%83.md.html" id="14 构建网络：一站式实现模型搭建与训练.md.html">14 构建网络：一站式实现模型搭建与训练.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/15%20%e5%8f%af%e8%a7%86%e5%8c%96%e5%b7%a5%e5%85%b7%ef%bc%9a%e5%a6%82%e4%bd%95%e5%ae%9e%e7%8e%b0%e8%ae%ad%e7%bb%83%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96%e7%9b%91%e6%8e%a7%ef%bc%9f.md.html" id="15 可视化工具：如何实现训练的可视化监控？.md.html">15 可视化工具：如何实现训练的可视化监控？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/16%20%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%ef%bc%9a%e5%a6%82%e4%bd%95%e5%8a%a0%e9%80%9f%e4%bd%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%ef%bc%9f.md.html" id="16 分布式训练：如何加速你的模型训练？.md.html">16 分布式训练：如何加速你的模型训练？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/17%20%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e5%8e%9f%e7%90%86%e4%b8%8e%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e6%a8%a1%e5%9e%8b.md.html" id="17 图像分类（上）：图像分类原理与图像分类模型.md.html">17 图像分类（上）：图像分类原理与图像分类模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/18%20%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e4%b8%80%e4%b8%aa%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e6%a8%a1%e5%9e%8b_.md.html" id="18 图像分类（下）：如何构建一个图像分类模型_.md.html">18 图像分类（下）：如何构建一个图像分类模型_.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/19%20%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e8%af%a6%e8%a7%a3%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%e5%8e%9f%e7%90%86%e4%b8%8e%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%e6%a8%a1%e5%9e%8b.md.html" id="19 图像分割（上）：详解图像分割原理与图像分割模型.md.html">19 图像分割（上）：详解图像分割原理与图像分割模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/20%20%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e4%b8%80%e4%b8%aa%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%e6%a8%a1%e5%9e%8b%ef%bc%9f.md.html" id="20 图像分割（下）：如何构建一个图像分割模型？.md.html">20 图像分割（下）：如何构建一个图像分割模型？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/21%20NLP%e5%9f%ba%e7%a1%80%ef%bc%88%e4%b8%8a%ef%bc%89%ef%bc%9a%e8%af%a6%e8%a7%a3%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e5%8e%9f%e7%90%86%e4%b8%8e%e5%b8%b8%e7%94%a8%e7%ae%97%e6%b3%95.md.html" id="21 NLP基础（上）：详解自然语言处理原理与常用算法.md.html">21 NLP基础（上）：详解自然语言处理原理与常用算法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/22%20NLP%e5%9f%ba%e7%a1%80%ef%bc%88%e4%b8%8b%ef%bc%89%ef%bc%9a%e8%af%a6%e8%a7%a3%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6.md.html" id="22 NLP基础（下）：详解语言模型与注意力机制.md.html">22 NLP基础（下）：详解语言模型与注意力机制.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/23%20%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90%ef%bc%9a%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8LSTM%e8%bf%9b%e8%a1%8c%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90%ef%bc%9f.md.html" id="23 情感分析：如何使用LSTM进行情感分析？.md.html">23 情感分析：如何使用LSTM进行情感分析？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/24%20%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%ef%bc%9a%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8BERT%e6%9e%84%e5%bb%ba%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e6%a8%a1%e5%9e%8b%ef%bc%9f.md.html" id="24 文本分类：如何使用BERT构建文本分类模型？.md.html">24 文本分类：如何使用BERT构建文本分类模型？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/25%20%e6%91%98%e8%a6%81%ef%bc%9a%e5%a6%82%e4%bd%95%e5%bf%ab%e9%80%9f%e5%ae%9e%e7%8e%b0%e8%87%aa%e5%8a%a8%e6%96%87%e6%91%98%e7%94%9f%e6%88%90%ef%bc%9f.md.html" id="25 摘要：如何快速实现自动文摘生成？.md.html">25 摘要：如何快速实现自动文摘生成？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/%e5%8a%a0%e9%a4%90%20%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%85%b6%e5%ae%9e%e5%b0%b1%e9%82%a3%e4%b9%88%e5%87%a0%e4%bb%b6%e4%ba%8b.md.html" id="加餐 机器学习其实就那么几件事.md.html">加餐 机器学习其实就那么几件事.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/%e7%94%a8%e6%88%b7%e6%95%85%e4%ba%8b%20Tango%ef%bc%9a%e5%b8%88%e5%82%85%e9%a2%86%e8%bf%9b%e9%97%a8%ef%bc%8c%e4%bf%ae%e8%a1%8c%e5%9c%a8%e4%b8%aa%e4%ba%ba.md.html" id="用户故事 Tango：师傅领进门，修行在个人.md.html">用户故事 Tango：师傅领进门，修行在个人.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/%e7%ad%94%e7%96%91%e7%af%87%20%e6%80%9d%e8%80%83%e9%a2%98%e7%ad%94%e6%a1%88%e9%9b%86%e9%94%a6.md.html" id="答疑篇 思考题答案集锦.md.html">答疑篇 思考题答案集锦.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/PyTorch%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98/%e7%bb%93%e6%9d%9f%e8%af%ad%20%e4%ba%ba%e7%94%9f%e5%85%85%e6%bb%a1%e9%80%89%e6%8b%a9%ef%bc%8c%e9%80%89%e6%8b%a9%e4%b8%8e%e5%8a%aa%e5%8a%9b%e5%90%8c%e6%a0%b7%e9%87%8d%e8%a6%81.md.html" id="结束语 人生充满选择，选择与努力同样重要.md.html">结束语 人生充满选择，选择与努力同样重要.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="21 NLP基础（上）：详解自然语言处理原理与常用算法" id="title">21 NLP基础（上）：详解自然语言处理原理与常用算法</h1>
<div><p>你好，我是方远。</p>
<p>在之前的课程中，我们一同学习了图像分类、图像分割的相关方法，还通过实战项目小试牛刀，学完这部分内容，相信你已经对深度学习图像算法有了一个较为深入的理解。</p>
<p>然而在实际的项目中，除了图像算法，还有一个大的问题类型，就是文字或者说语言相关的算法。这一类让程序理解人类语言表达的算法或处理方法，我们统称为自然语言处理（Natural Language Processing, NLP）。</p>
<p>这节课，我们先来学习自然语言处理的原理和常用算法，通过这一部分的学习，以后你遇到一些常见的NLP问题，很容易就能想出自己的解决办法。不必担心原理、算法的内容太过理论化，我会结合自己的经验从实际应用的角度，为你建立对NLP的整体认知。</p>
<h2 id="nlp的应用无处不在">NLP的应用无处不在</h2>
<p>NLP研究的领域非常广泛，凡是跟语言学有关的内容都属于NLP的范畴。一般来说，较为多见的语言学的方向包括：词干提取、词形还原、分词、词性标注、命名实体识别、语义消歧、句法分析、指代消解、篇章分析等方面。</p>
<p>看到这里，你可能感觉这些似乎有点太学术、太专业了，涉及语言的结构、构成甚至是性质方面的研究了。没错，这些都是NLP研究在语言学中的应用方面，就会给人一种比较偏研究的感觉。</p>
<p>实际上，NLP还有很多的研究内容是侧重“处理”和“应用”方面的，比如我们常见的就有：机器翻译、文本分类、问答系统、知识图谱、信息检索等等。</p>
<p><img alt="图片" src="assets/3250830f7d954fa48bd98012be7e2b2b.jpg"/></p>
<p>我举一个例子，你就知道自然语言处理有多么重要了。平时我们经常会用搜索引擎，当你打开网页、在搜索框中输入自己想要了解的关键词之后，搜索引擎的后台算法逻辑就要开始一整套非常复杂的算法逻辑，这其中包括几个比较重要的方面，我们不妨结合例子来看看。</p>
<p>在搜索引擎的输入框中，输入“亚洲的动wu”文本，显示的内容如下图所示。别看只是一次简单的检索动作，搜索系统要完成的工作可不少。</p>
<p><img alt="图片" src="assets/cc04e10c307447ef82e72d2ca8b9eae9.jpg"/></p>
<p>首先，搜索引擎要对你输入的内容（query）进行解析，这就涉及到了之前提到的分词、命名实体识别、语义消歧等内容，当然还涉及到了query纠错，因为你错误地输入了拼音而非汉字，需要改写成正确的形式。</p>
<p>通过一系列的算法之后，系统识别出你的需求是：寻找动物相关的搜索结果，这些结果的限定条件是它们要生活在亚洲。</p>
<p>接着，系统就开始在数据库（或者是存储的集群中）搜索相关的实体，这些实体的查询和限制条件的过滤，就涉及信息检索、知识图谱等内容。</p>
<p>最后，细心的同学对照搜索结果会发现，有的时候搜索引擎除了提供严格匹配的检索结果之外，还会提供一些相关内容的扩展结果，比如广告、新闻、视频等。而且很多搜索引擎的扩展搜索结果页都是个性化的，也就是根据用户的特点行为提供推荐，这些让我们的搜索结果更加丰富，体验更好。</p>
<p>仅仅只有这些了么？不，远远没有，因为刚才的这个过程，只是针对你这一个用户的一次检索所需要完成的一部分工作而已。更多的工作，实际是用户开始使用搜索引擎之前的构建准备阶段。</p>
<p>为了构建搜索引擎，就需要对存储的内容进行解析，这就包括了篇章理解、文本处理、图片识别、音视频算法等环节，对每一个网页（内容）进行特征的提取，构建检索库、知识库等，这个工作量就会非常的大，涉及的面也非常广泛。</p>
<p>由此可见，NLP的应用真的深入到了互联网业务的方方面面，掌握了NLP的相关算法将会使我们的竞争力变得更强。接下来，针对自然语言处理的“应用”方面，我们一起聊聊NLP中文场景下的一些重要内容。</p>
<h2 id="nlp的几个重要内容">NLP的几个重要内容</h2>
<p>想要让程序对文本内容进行理解，我们需要解决几个非常基础和重要的内容，分别是分词、文本表示以及关键词提取。</p>
<h3 id="分词">分词</h3>
<p>中文跟英文最大的不同在于，英文是由一个个单词构成的，单词与单词之间有空格隔断。但是中文不一样，中文单词和单词之间除了标点符号没有别的隔断。这就给程序理解文本带来了一定的难度，分词的需求也应运而生。</p>
<p>尽管现在的深度学习已经对分词的依赖越来越小，可以通过Word Embedding等方式对字符（token）级的文字进行表示，但是分词的地位不会降低，单词、词组级别的文本表示仍旧有非常多的应用场景。</p>
<p>因为我们的学习重在快速上手和实战应用，所以为了降低你的学习成本，这个专栏里我不会专门深入讲解各种分词算法细节，而是侧重于带你理解其特点，并教你学习如何用相应的工具包实现分词过程。</p>
<p>目前网络上已经有了很多的开源或者免费的NLP分词工具，比如jieba、HanLP、THULAC等，包括腾讯、百度、阿里等公司也有相应的商业付费工具。</p>
<p>贫穷使人理智，我们今天使用免费的jieba分词来做一个分词的例子，链接你可以从<a href="https://pypi.org/project/jieba" target="_blank">这里</a>获取。安装这个工具非常简单，只需要使用pip即可。</p>
<pre><code class="language-python">pip install jieba
</code></pre>
<p>jieba的使用也很方便，我来演示一下：</p>
<pre><code class="language-python">import jieba
text = "极客时间棒呆啦"
# jieba.cut得到的是generator形式的结果
seg = jieba.cut(text)  
print(' '.join(seg)) 

# Get： 极客 时间 棒呆 啦
</code></pre>
<p>其实除了分词，jieba还提供了词性标注的结果（pos）：</p>
<pre><code class="language-python">import jieba.posseg as posseg
text = "一天不看极客时间我就浑身难受"
# 形如pair('word, 'pos')的结果
seg = posseg.cut(text)  
print([se for se in seg]) 
# Get [pair('一天', 'm'), pair('不', 'd'), pair('看', 'v'), pair('极客', 'n'), pair('时间', 'n'), pair('我', 'r'), pair('就', 'd'), pair('浑身', 'n'), pair('难受', 'v')]
</code></pre>
<p>是不是非常简单？搞定了分词，我们接下来就要开始对文本进行表示了。</p>
<h3 id="文本表示的方法">文本表示的方法</h3>
<p>在深度学习被广泛采用之前，很多传统机器学习算法结合自身的特点，使用了各种各样的文本表示。</p>
<p>最经典的就是独热（One-hot）表示法了。在这种方法中，假定所有的文字一共有N个单词（也可以是字符），我们可以将每个单词赋予一个单独的序号id，那么对于任意一个单词，我们都可以采用一个N位的列表（向量）对其进行表示。在这个表示中，只需要将这个单词对应序号id的位置为1，其他位置为0即可。</p>
<p>我还是举个例子来帮你加深理解。比方说，我们词典大小为10000，“极客”这个单词的序号id为666，那么我们就需要建立一个10000长度的向量，并将其中的第666位置为1，其余位为0。如下：</p>
<p><img alt="图片" src="assets/1c06815c32a44579ab9d8f2a86d43201.jpg"/></p>
<p>这时候你就会发现，在UTF-8编码中，中文字符有两万多个，词语数量更是天文数字，那么我们仅用字符的方式，每个字就需要两万多维度的数据来表示。推算一下，如果有一篇一万字的文章，这个数据量就很可怕了。</p>
<p>为了进一步的压缩数据的体积，可以只使用一个向量表示文章中所有的单词，例如前面的例子，我们仍旧建立一个10000维的向量，把文章中出现过的所有单词的对应位置置为1，其余为0。</p>
<p><img alt="图片" src="assets/418e4bb6fbfd47dca8753f996ef213d5.jpg"/></p>
<p>这样看上去，数据体积就少了很多，还有没有其他办法进一步缩减空间的占用呢？有的，例如count-based表示方法。</p>
<p>在这种方法中，我们采用v={index1: count1, index2: count2,…, index n: count n}的形式，对每一个出现的单词的序号id以及出现过的次数进行统计，这样一来，“极客时间”我们只需要两个k-v对的dict即可表示: {3:1, 665:1}。</p>
<p>这种表示方法在SVM、树模型等多个算法包中被广泛采用，因为客观来说，<strong>它确实能够大幅度地压缩空间的占用，生成起来也非常方便</strong>。但是你会发现前面这几种方法，不能表述单词的语序信息。</p>
<p>举个例子，“我/喜欢/你”和“你/喜欢/我”两个截然不同的意思，用前面的方法做分词的话，却会得到相同的表示结果。这时候如果搞错了，其实却是单相思的话，那岂不是很苦涩？</p>
<p>好在现在深度学习的使用推动了Word Embedding的发展，基本上我们都会采用该方法进行文本表示。但是还是刚才的话，这并不意味着传统的文本表示方法就过时了。在一些小规模、轻量级的文本处理场景中，它们的作用仍旧非常大。</p>
<p>关于文本表示中Word Embedding的部分，咱们在后续课程再展开讲解，这也是NLP深度学习的核心内容之一。</p>
<p>让我们回到刚才的传统文本表示方法，为了实现对单词顺序信息的记录，该怎么办呢？这时我们要解决NLP中的一个重要问题：关键词的提取。</p>
<h3 id="关键词的提取">关键词的提取</h3>
<p>关键词，顾名思义，就是能够表达文本中心内容的词语。关键词提取在检索系统、推荐系统等应用中有着极重要的地位。它是文本数据挖掘领域的一个分支，所以在摘要生成、文本分类/聚类等领域中也是非常基础的环节。</p>
<p>关键词提取，主要分为有监督和无监督的方法，一般来说，我们采用无监督的方法较多一些，这是因为它不需要人工标注的语料，只需要对文本中的单词按照位置、频率、依存关系等信息进行判断，就可以实现关键词的识别和提取。</p>
<p>无监督方法一般有三种类型，基于统计特征的方法、基于词图模型的方法，以及基于主题模型的方法，我们分别来看看。</p>
<h4 id="基于统计特征的方法">基于统计特征的方法</h4>
<p>这种类型的方法最为经典的就是TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）。该方法最核心的思想非常简单：一个单词在文件中出现的次数越多，它的重要性越高；但它在语料库中出现的频率越高，它的重要性反而越小。</p>
<p>什么意思呢？就比如说我们有10篇文章，其中有2篇财经文章、5篇科技、3篇娱乐，对于单词“股票”，它在财经文章中的次数肯定非常多，但是在娱乐和科技中就非常少，这就意味着“股票”这个词就能够更好的“区分”文章的类别，那它的重要性自然也就非常高了。</p>
<p>在TF-IDF中，词频（TF）表示关键字在文本中出现的频率。而逆向文件频率 (IDF) 是由<strong>包含该词语的文件的数目</strong>除以<strong>总文件数目</strong>得到的，一般情况下还会取对数对结果进行缩放。</p>
<p><img alt="图片" src="assets/300848ca7f2748e1aa243c3263d086f7.jpg"/>-
<img alt="图片" src="assets/9d085324d4d946c29249b7caa2638be7.jpg"/></p>
<p>你可以自己先想想，这里为什么分母要加1呢？这是为了避免分母为0的情况。得到了TF和IDF之后，我们将两者相乘，就得到了TF-IDF了。</p>
<p>通过TF-IDF不难看出，基于统计的方法的特点在于，<strong>对单词出现的次数以及分布进行数学上的统计，从而发现相应的规律和重要性（权重），并以此作为关键词提取的依据</strong>。</p>
<p>跟分词一样，关键词的提取目前也有很多集成工具包，比如NLTK（Natural Language Toolkit），它是一个非常著名的自然语言处理工具包，是NLP研究领域常用的Python库。我们仍旧可以使用pip install nltk命令来进行安装。</p>
<p>使用NLTK来计算TF-IDF非常简单，代码如下：</p>
<pre><code class="language-python">from nltk import word_tokenize
from nltk import TextCollection

sents=['i like jike','i want to eat apple','i like lady gaga']
# 首先进行分词
sents=[word_tokenize(sent) for sent in sents]

# 构建语料库
corpus=TextCollection(sents)

# 计算TF
tf=corpus.tf('one',corpus)

# 计算IDF
idf=corpus.idf('one')

# 计算任意一个单词的TF-IDF
tf_idf=corpus.tf_idf('one',corpus)
</code></pre>
<p>你可以执行前面这段代码，看看tf_idf等于多少？</p>
<h4 id="基于词图模型的关键词提取"><strong>基于词图模型的关键词提取</strong></h4>
<p>前面基于统计的方法采用的是对词语的频率计算的方式，但我们还可以有其他的提取思路，那就是基于词图模型的关键词提取。</p>
<p>在这种方法中，我们首先要构建文本一个图结构，用来表示语言的词语网络。然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，即关键词。</p>
<p>该类方法中最经典的就是TextRank算法了，它脱胎于更为经典的网页排序算法PageRank。关于PageRank算法，你可以参考这个wiki（<a href="https://en.wikipedia.org/wiki/PageRank" target="_blank">戳我</a>）。戳完PageRank之后，你就会知道，PageRank算法的核心内容有两点：</p>
<ul>
<li><p>如果一个网页被很多其他网页链接到的话，就说明这个网页比较重要，也就是PageRank值会相对较高。</p></li>
<li><p>如果一个PageRank值很高的网页，链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。-
而TextRank就非常好理解了。它跟PageRank的区别在于：</p></li>
<li><p>用句子代替网页</p></li>
<li><p>任意两个句子的相似性可以采用类似网页转换概率的概念计算，但是也稍有不同，TextRank用归一化的句子相似度代替了PageRank中相等的转移概率，所以在TextRank中，所有节点的转移概率不会完全相等。</p></li>
<li><p>利用矩阵存储相似性的得分，类似于PageRank的矩阵M。</p></li>
<li><p>TextRank的基本流程如下图所示。</p></li>
</ul>
<p><img alt="图片" src="assets/decf7fc14bcb44e39be07ae9d040172f.jpg"/></p>
<p>看上去蛮复杂的，不过没有关系，刚才提到的jieba也有了相应的集成算法。在jieba中，我们可以使用如下的函数进行提取：</p>
<pre><code class="language-python">jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False)
</code></pre>
<p>其中sentence是待处理的文本，topK是选择最重要的K个关键词，基本上你用好这两个参数就足够了。</p>
<h4 id="基于主题模型的关键词提取">基于主题模型的关键词提取</h4>
<p>最后一种关键词提取方法就是基于主题模型的关键词提取。</p>
<p>主题模型，这个名字看起来就高端了很多，实际上它也是一种基于统计的模型，只不过它会“发现”文档集合中出现的抽象的“主题”，并用于挖掘文本中隐藏的语义结构。</p>
<p>LDA（Latent Dirichlet Allocation）文档主题生成模型，是最典型的基于主题模型的算法。有关LDA的算法的介绍，你随便搜索一下网络资料就能找到，我就不展开说了。而咱们在这节课中，将会利用已经集成好的工具包gensim来实现使用这个模型，代码也非常简单，我们一起来看一下：</p>
<pre><code class="language-python">from gensim import corpora, models
import jieba.posseg as jp
import jieba

input_content = [line.strip() for line in open ('input.txt', 'r')]
# 老规矩，先分词
words_list = []
for text in input_content:
  words = [w.word for w in jp.cut(text)]
  words_list.append(words)

# 构建文本统计信息, 遍历所有的文本，为每个不重复的单词分配序列id，同时收集该单词出现的次数
dictionary = corpora.Dictionary(words_list)

# 构建语料，将dictionary转化为一个词袋。
# corpus是一个向量的列表，向量的个数就是文档数。你可以输出看一下它内部的结构是怎样的。
corpus = [dictionary.doc2bow(words) for words in words_list]

# 开始训练LDA模型
lda_model = models.ldamodel.LdaModel(corpus=corpus, num_topics=8, id2word=dictionary, passes=10)
</code></pre>
<p>在训练环节中，num_topics代表生成的主题的个数。id2word即为dictionary，它把id都映射成为字符串。passes相当于深度学习中的epoch，表示模型遍历语料库的次数。</p>
<h2 id="小结"><strong>小结</strong></h2>
<p>在这节课中，我带你一同了解了自然语言处理的应用场景以及三个经典的NLP基础问题。</p>
<p>NLP的三大经典问题包括分词、文本表示、关键词提取，正是因为这三个问题太过经典和基础，所以现在已经有了大量的集成工具供我们直接使用。</p>
<p>但我还是那句话，有了工具，并不意味着我们不需要理解它内部的原理，学习要知其然，更需要知其所以然，这样在实际的工作中遇到问题的时候，我们才能游刃有余地解决。</p>
<p>细心的你可能已经发现了，今天的课程里我们针对不同的问题使用了不同的工具包，分词我们使用了jieba，关键词的提取我们使用了gensim和NLTK，所以我希望你在课后有空的时候，也去了解一下这三个工具的具体使用和更多功能，因为它们真的很强大。</p>
<p>在文本表示方法中，我们留了一个小尾巴，也就是Word Embedding。随着深度学习的越来越广泛使用，词嵌入（Word Embedding）的方法也有了越来越多的算法和工具来实现。在后续的课程中，我会通过BERT的实战开发来向你介绍Word Embedding的训练生成和使用。</p>
<h2 id="每课一练">每课一练</h2>
<p>TF-IDF有哪些缺点呢？你不妨结合它的计算过程做个梳理。</p>
<p>期待你在留言区跟我交流互动，也推荐你把这节课分享给身边对NLP感兴趣的同事、朋友，跟他一起学习进步。</p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#761a1a1a4f424747464136111b171f1a5815191b" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'935872dfdf3942f1',t:'MTc0NTUyNzU3Mi4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>