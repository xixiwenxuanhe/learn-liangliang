<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="19  逻辑回归：如何让计算机做出二值化决策？" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>19  逻辑回归：如何让计算机做出二值化决策？ </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%20%e6%95%b0%e5%ad%a6%ef%bc%8c%e7%bc%96%e7%a8%8b%e8%83%bd%e5%8a%9b%e7%9a%84%e8%90%a5%e5%85%bb%e6%a0%b9%e5%9f%ba.md.html" id="00 开篇词  数学，编程能力的营养根基.md.html">00 开篇词  数学，编程能力的营养根基.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/01%20%20%e4%bb%8e%e8%ae%a1%e6%95%b0%e5%bc%80%e5%a7%8b%ef%bc%8c%e7%a8%8b%e5%ba%8f%e5%91%98%e5%bf%85%e7%9f%a5%e5%bf%85%e4%bc%9a%e7%9a%84%e6%95%b0%e5%88%b6%e8%bd%ac%e6%8d%a2%e6%b3%95.md.html" id="01  从计数开始，程序员必知必会的数制转换法.md.html">01  从计数开始，程序员必知必会的数制转换法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/02%20%20%e9%80%bb%e8%be%91%e4%b8%8e%e6%b2%9f%e9%80%9a%ef%bc%8c%e6%80%8e%e6%a0%b7%e6%89%8d%e8%83%bd%e8%ae%b2%e5%87%ba%e6%9c%89%e9%80%bb%e8%be%91%e7%9a%84%e8%af%9d%ef%bc%9f.md.html" id="02  逻辑与沟通，怎样才能讲出有逻辑的话？.md.html">02  逻辑与沟通，怎样才能讲出有逻辑的话？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/03%20%20%e7%94%a8%e6%95%b0%e5%ad%a6%e5%86%b3%e7%ad%96%ef%bc%8c%e5%a6%82%e4%bd%95%e8%a7%84%e5%88%92%e5%a5%bd%e6%8a%95%e5%85%a5%e3%80%81%e8%bd%ac%e5%8c%96%e5%92%8c%e4%ba%a7%e5%87%ba%ef%bc%9f.md.html" id="03  用数学决策，如何规划好投入、转化和产出？.md.html">03  用数学决策，如何规划好投入、转化和产出？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/04%20%20%e4%b8%87%e7%89%a9%e5%8f%af%e6%95%b0%e5%ad%a6%ef%bc%8c%e7%bb%8f%e5%85%b8%e5%85%ac%e5%bc%8f%e6%98%af%e5%a6%82%e4%bd%95%e5%9c%a8%e7%94%9f%e6%b4%bb%e4%b8%ad%e5%ba%94%e7%94%a8%e7%9a%84%ef%bc%9f.md.html" id="04  万物可数学，经典公式是如何在生活中应用的？.md.html">04  万物可数学，经典公式是如何在生活中应用的？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/05%20%20%e6%b1%82%e6%9e%81%e5%80%bc%ef%bc%9a%e5%a6%82%e4%bd%95%e6%89%be%e5%88%b0%e5%a4%8d%e6%9d%82%e4%b8%9a%e5%8a%a1%e7%9a%84%e6%9c%80%e4%bc%98%e8%a7%a3%ef%bc%9f.md.html" id="05  求极值：如何找到复杂业务的最优解？.md.html">05  求极值：如何找到复杂业务的最优解？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/06%20%20%e5%90%91%e9%87%8f%e5%8f%8a%e5%85%b6%e5%af%bc%e6%95%b0%ef%bc%9a%e8%ae%a1%e7%ae%97%e6%9c%ba%e5%a6%82%e4%bd%95%e5%ae%8c%e6%88%90%e5%af%b9%e6%b5%b7%e9%87%8f%e9%ab%98%e7%bb%b4%e5%ba%a6%e6%95%b0%e6%8d%ae%e8%ae%a1%e7%ae%97%ef%bc%9f.md.html" id="06  向量及其导数：计算机如何完成对海量高维度数据计算？.md.html">06  向量及其导数：计算机如何完成对海量高维度数据计算？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/07%20%20%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%9a%e5%a6%82%e4%bd%95%e5%9c%a8%e7%a6%bb%e6%95%a3%e7%82%b9%e4%b8%ad%e5%af%bb%e6%89%be%e6%95%b0%e6%8d%ae%e8%a7%84%e5%be%8b%ef%bc%9f.md.html" id="07  线性回归：如何在离散点中寻找数据规律？.md.html">07  线性回归：如何在离散点中寻找数据规律？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/08%20%20%e5%8a%a0%e4%b9%98%e6%b3%95%e5%88%99%ef%bc%9a%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e5%a4%8d%e6%9d%82%e4%ba%8b%e4%bb%b6%e5%8f%91%e7%94%9f%e7%9a%84%e6%a6%82%e7%8e%87%ef%bc%9f.md.html" id="08  加乘法则：如何计算复杂事件发生的概率？.md.html">08  加乘法则：如何计算复杂事件发生的概率？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/09%20%20%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1%ef%bc%9a%e5%a6%82%e4%bd%95%e5%88%a9%e7%94%a8%20MLE%20%e5%af%b9%e5%8f%82%e6%95%b0%e8%bf%9b%e8%a1%8c%e4%bc%b0%e8%ae%a1%ef%bc%9f.md.html" id="09  似然估计：如何利用 MLE 对参数进行估计？.md.html">09  似然估计：如何利用 MLE 对参数进行估计？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/10%20%20%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9a%e4%ba%8b%e4%bb%b6%e7%9a%84%e4%b8%8d%e7%a1%ae%e5%ae%9a%e6%80%a7%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%ef%bc%9f.md.html" id="10  信息熵：事件的不确定性如何计算？.md.html">10  信息熵：事件的不确定性如何计算？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/11%20%20%e7%81%b0%e5%ba%a6%e5%ae%9e%e9%aa%8c%ef%bc%9a%e5%a6%82%e4%bd%95%e8%ae%be%e8%ae%a1%e7%81%b0%e5%ba%a6%e5%ae%9e%e9%aa%8c%e5%b9%b6%e8%ae%a1%e7%ae%97%e5%ae%9e%e9%aa%8c%e7%9a%84%e6%94%b6%e7%9b%8a%ef%bc%9f.md.html" id="11  灰度实验：如何设计灰度实验并计算实验的收益？.md.html">11  灰度实验：如何设计灰度实验并计算实验的收益？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/12%20%20%e7%bb%9f%e8%ae%a1%e5%ad%a6%e6%96%b9%e6%b3%95%ef%bc%9a%e5%a6%82%e4%bd%95%e8%af%81%e6%98%8e%e7%81%b0%e5%ba%a6%e5%ae%9e%e9%aa%8c%e6%95%88%e6%9e%9c%e4%b8%8d%e6%98%af%e5%81%b6%e7%84%b6%e5%be%97%e5%88%b0%e7%9a%84%ef%bc%9f.md.html" id="12  统计学方法：如何证明灰度实验效果不是偶然得到的？.md.html">12  统计学方法：如何证明灰度实验效果不是偶然得到的？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/13%20%20%e5%a4%8d%e6%9d%82%e5%ba%a6%ef%bc%9a%e5%a6%82%e4%bd%95%e5%88%a9%e7%94%a8%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e5%af%b9%e7%a8%8b%e5%ba%8f%e8%bf%9b%e8%a1%8c%e4%bc%98%e5%8c%96%ef%bc%9f.md.html" id="13  复杂度：如何利用数学推导对程序进行优化？.md.html">13  复杂度：如何利用数学推导对程序进行优化？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/14%20%20%e7%a8%8b%e5%ba%8f%e7%9a%84%e5%be%aa%e7%8e%af%ef%bc%9a%e5%a6%82%e4%bd%95%e5%88%a9%e7%94%a8%e6%95%b0%e5%ad%a6%e5%bd%92%e7%ba%b3%e6%b3%95%e8%bf%9b%e8%a1%8c%e7%a8%8b%e5%ba%8f%e5%bc%80%e5%8f%91%ef%bc%9f.md.html" id="14  程序的循环：如何利用数学归纳法进行程序开发？.md.html">14  程序的循环：如何利用数学归纳法进行程序开发？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/15%20%20%e9%80%92%e5%bd%92%ef%bc%9a%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e6%b1%89%e8%af%ba%e5%a1%94%e9%97%ae%e9%a2%98%e7%9a%84%e7%a7%bb%e5%8a%a8%e6%ad%a5%e6%95%b0%ef%bc%9f.md.html" id="15  递归：如何计算汉诺塔问题的移动步数？.md.html">15  递归：如何计算汉诺塔问题的移动步数？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/16%20%20%e4%ba%8c%e5%88%86%e6%b3%95%ef%bc%9a%e5%a6%82%e4%bd%95%e5%88%a9%e7%94%a8%e6%8c%87%e6%95%b0%e7%88%86%e7%82%b8%e4%bc%98%e5%8c%96%e7%a8%8b%e5%ba%8f%ef%bc%9f.md.html" id="16  二分法：如何利用指数爆炸优化程序？.md.html">16  二分法：如何利用指数爆炸优化程序？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/17%20%20%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92%ef%bc%9a%e5%a6%82%e4%bd%95%e5%88%a9%e7%94%a8%e6%9c%80%e4%bc%98%e5%ad%90%e7%bb%93%e6%9e%84%e8%a7%a3%e5%86%b3%e9%97%ae%e9%a2%98%ef%bc%9f.md.html" id="17  动态规划：如何利用最优子结构解决问题？.md.html">17  动态规划：如何利用最优子结构解决问题？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/18%20%20AI%20%e5%85%a5%e9%97%a8%ef%bc%9a%e5%88%a9%e7%94%a8%203%20%e4%b8%aa%e5%85%ac%e5%bc%8f%e6%90%ad%e5%bb%ba%e6%9c%80%e7%ae%80%20AI%20%e6%a1%86%e6%9e%b6.md.html" id="18  AI 入门：利用 3 个公式搭建最简 AI 框架.md.html">18  AI 入门：利用 3 个公式搭建最简 AI 框架.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/19%20%20%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%ef%bc%9a%e5%a6%82%e4%bd%95%e8%ae%a9%e8%ae%a1%e7%ae%97%e6%9c%ba%e5%81%9a%e5%87%ba%e4%ba%8c%e5%80%bc%e5%8c%96%e5%86%b3%e7%ad%96%ef%bc%9f.md.html" id="19  逻辑回归：如何让计算机做出二值化决策？.md.html">19  逻辑回归：如何让计算机做出二值化决策？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/20%20%20%e5%86%b3%e7%ad%96%e6%a0%91%ef%bc%9a%e5%a6%82%e4%bd%95%e5%af%b9%20NP%20%e9%9a%be%e5%a4%8d%e6%9d%82%e9%97%ae%e9%a2%98%e8%bf%9b%e8%a1%8c%e5%90%af%e5%8f%91%e5%bc%8f%e6%b1%82%e8%a7%a3%ef%bc%9f.md.html" id="20  决策树：如何对 NP 难复杂问题进行启发式求解？.md.html">20  决策树：如何对 NP 难复杂问题进行启发式求解？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/21%20%20%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%8e%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%ef%bc%9a%e8%ae%a1%e7%ae%97%e6%9c%ba%e6%98%af%e5%a6%82%e4%bd%95%e7%90%86%e8%a7%a3%e5%9b%be%e5%83%8f%e3%80%81%e6%96%87%e6%9c%ac%e5%92%8c%e8%af%ad%e9%9f%b3%e7%9a%84%ef%bc%9f.md.html" id="21  神经网络与深度学习：计算机是如何理解图像、文本和语音的？.md.html">21  神经网络与深度学习：计算机是如何理解图像、文本和语音的？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/22%20%20%e9%9d%a2%e8%af%95%e4%b8%ad%e9%82%a3%e4%ba%9b%e5%9d%91%e4%ba%86%e6%97%a0%e6%95%b0%e4%ba%ba%e7%9a%84%e7%ae%97%e6%b3%95%e9%a2%98.md.html" id="22  面试中那些坑了无数人的算法题.md.html">22  面试中那些坑了无数人的算法题.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/23%20%20%e7%ab%99%e5%9c%a8%e7%94%9f%e6%b4%bb%e7%9a%84%e5%8d%81%e5%ad%97%e8%b7%af%e5%8f%a3%ef%bc%8c%e5%a6%82%e4%bd%95%e7%94%a8%e6%95%b0%e5%ad%a6%e6%8a%89%e6%8b%a9%ef%bc%9f.md.html" id="23  站在生活的十字路口，如何用数学抉择？.md.html">23  站在生活的十字路口，如何用数学抉择？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e7%a8%8b%e5%ba%8f%e5%91%98%e7%9a%84%e6%95%b0%e5%ad%a6%e8%af%be/24%20%e7%bb%93%e6%9d%9f%e8%af%ad%20%20%e6%95%b0%e5%ad%a6%e5%ba%95%e5%ad%90%e5%a5%bd%ef%bc%8c%e5%ad%a6%e5%95%a5%e9%83%bd%e5%bf%ab.md.html" id="24 结束语  数学底子好，学啥都快.md.html">24 结束语  数学底子好，学啥都快.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="19  逻辑回归：如何让计算机做出二值化决策？" id="title">19  逻辑回归：如何让计算机做出二值化决策？</h1>
<div><p>在上一讲，学习完 AI 的基本框架后，我们现在就开始围绕当前人工智能领域最常用的模型，来分别学习一下它们背后的原理。</p>
<p>这一讲，我们从最常见的逻辑回归模型说起，逻辑回归是人工智能领域中入门级的基础模型，它在很多领域都有应用，例如用户的信贷模型、疾病识别等。</p>
<p>逻辑回归是一种分类模型，可以对一个输入 x，识别并预测出一个二值化的类别标签 y。例如，要预测照片中人物的性别，可以采用逻辑回归建立模型。给模型输入一个描述照片的特征向量 x，经过模型的计算，可以得到输出值 y 为“男”或“女”。</p>
<p>在深入学习逻辑回归的原理之前，我们先来了解一下什么是分类问题，以及分类问题有哪些类型。</p>
<h3 id="分类问题">分类问题</h3>
<p>在人工智能领域中，分类问题是特别常见的一种问题类型。简而言之，分类问题就是对一个测试验本去预测它归属的类别。例如，预测胎儿性别、预测足球比赛结果。</p>
<p>根据归属类别可能性的数量，分类问题又可以分为二分类问题和多分类问题。</p>
<ul>
<li>二分类问题，顾名思义就是预测的归属类别只有两个。例如，预测性别男/女、预测主场球队的胜负、预测明天是否下雨。</li>
<li>多分类问题，预测的归属类别大于两个的那类问题。例如，预测足球比赛结果是胜、负，还是平局；预测明天天气是雨天、晴天，还是阴天。</li>
</ul>
<p>在研究分类的建模算法时，人们往往会从二分类问题入手，这主要是因为多分类问题可以用多个二分类问题来表示。例如，预测明天天气是雨天、晴天，还是阴天，这是个多分类问题（三分类）；它也可以表示为，预测明天是否下雨、预测明天是否晴天、预测明天是否阴天，这三个二分类问题。</p>
<p>因此，二分类问题是分类问题的基础，在讨论分类算法时，人们往往会从二分类问题入手。</p>
<h3 id="逻辑回归及其建模流程">逻辑回归及其建模流程</h3>
<p>逻辑回归（Logistic Regression，LR）是人工智能领域非常经典的算法之一，它可以用来对二分类问题进行建模，对于一个给定的输入，可以预测其类别为正 1 或负 0。接下来，我们就从 AI 基本框架的 3 个公式，来学习一下 LR 的建模流程。</p>
<p>重温一下人工智能基本框架的 3 个公式分别是：</p>
<ul>
<li>第一步，根据假设，写出模型的输入、输出关系 y = f(<strong><em>w</em></strong>; x)；</li>
<li>第二步，根据偏差的计算方法，写出描述偏差的损失函数 L(<strong><em>w</em></strong>)；</li>
<li>第三步，对于损失函数，求解最优的参数值，即 <strong><em>w</em></strong>*= argmin L(<strong><em>w</em></strong>)。</li>
</ul>
<p>接下来，我会逐一展示这三步的过程。</p>
<h4 id="1-模型的输入-输出关系-sigmoid-函数">1.模型的输入、输出关系（Sigmoid 函数）</h4>
<p>在逻辑回归中，第一个公式的表达式非常简单，为 y=f(<strong><em>w;x</em></strong>)=sigmoid(<strong><em>w·x</em></strong>)=1/(1+e-<strong><em>w·x</em></strong>)。</p>
<p>直观上来看，逻辑回归的模型假设是，把模型参数向量 <strong><em>w</em></strong> 和输入向量 <strong><em>x</em></strong> 的点乘（即线性变换）结果输入给 Sigmoid 函数中，即可得到预测值 y。</p>
<p>此时的预测值 y 还是个 0～1 之间的连续值，这是因为 Sigmoid 函数的值域是 (0,1)。逻辑回归是个二分类模型，它的最终输出值只能是两个类别标签之一。通常，我们习惯于用“0”和“1”来分别标记二分类的两个类别。</p>
<p>在逻辑回归中，常用预测值 y 和 0.5 的大小关系，来判断样本的类别归属。<strong>具体地，预测值 y 如果大于 0.5，则认为预测的类别为 1；反之，则预测的类别为 0。</strong></p>
<p>我们把上面的描述进行总结，来汇总一下逻辑回归输入向量、预测值和类别标签之间的关系，则有下面的流程图。</p>
<p><img alt="图片1.png" src="assets/Cip5yF_lxJuAJjk2AAE7vKJA3Cg656.png"/></p>
<p>为了深入了解逻辑回归的模型假设，我们需要先认识下 Sigmoid 函数。Sigmoid 函数的表达式为 y = sigmoid(x)=1/(1+e-x)，它是个单调递增函数，定义域为 (-∞, +∞)，值域为 (0,1)，它的函数图像如下。
<img alt="图片2.png" src="assets/CgqCHl_lxKuAaTt2AAEuW5MrG8c228.png"/>
我们可以看出，Sigmoid 函数可以将任意一个实数 x，单调地映射到 0 到 1 的区间内，这正好符合了“概率”的取值范围。</p>
<p>我们还可以用求导公式来看一下 Sigmoid 函数的一阶导数。
<img alt="WechatIMG1422.png" src="assets/Cip5yF_pVFqAQwAPAAA8cpQoHKA089.png"/></p>
<h4 id="2-逻辑回归的损失函数">2.逻辑回归的损失函数</h4>
<p>有了这些基本假设后，我们尝试根据偏差的计算方法，写出描述偏差的损失函数 L(<strong><em>w</em></strong>)。</p>
<p>我们刚刚提到过，逻辑回归预测结果的值域 y 为 (0,1)，代表的是样本属于类别 1 的概率。</p>
<ul>
<li>具体而言，如果样本属于类别“1”的概率大于 0.5，则认为样本的预测类别为“1”；</li>
<li>如果样本属于类别“1”的概率小于 0.5，则认为样本的预测类别为“0”。</li>
</ul>
<p>这里出现了这么多的概率，我们可以借鉴在《09 | 似然估计：如何利用 MLE 对参数进行估计？》中学的概率计算和极大似然估计的思想，尝试写出样本被正确预测的概率。
<img alt="WechatIMG1421.png" src="assets/CgqCHl_pVHaAOfpZAAC78iA7-nA342.png"/>
我们将上面两个等式合并，就可以得到某个数据<strong><em>x</em></strong>i 被正确预测的概率，即 P(yi|xi,w)=Φ(zi)yi·[1-Φ(zi)]1-yi。</p>
<ul>
<li>如果真实结果 yi 为 1，则 P(yi|xi,w) = Φ(zi)，描述的是样本被预测为类别“1”的概率；</li>
<li>如果真实结果 yi 为 0，则 P(yi|xi,w) = 1-Φ(zi)，描述的是样本被预测为类别“0”的概率。</li>
</ul>
<p>接下来可以将上式扩展到整个样本数据集中，则可采用极大似然估计得到 L(<strong><em>w</em></strong>)，即
<img alt="图片4.png" src="assets/Ciqc1F_lxSyACUiYAAA4FTEM9qs111.png"/></p>
<p>我们之前在《09 | 似然估计：如何利用 MLE 对参数进行估计？》学习极大似然估计 MLE 时，曾经提过一个常用的公式化简方法，那就是通过取对数，让连续乘积的大型运算变为连续求和，则有
<img alt="图片5.png" src="assets/CgpVE1_lxTyASfq6AAA1M4H_6RI019.png"/></p>
<h4 id="3-求解最优的模型参数值"><strong>3.求解最优的模型参数值</strong></h4>
<p>AI 建模框架的最后一步，就是对损失函数求解最优的参数值，即<strong><em>w</em></strong>*= argmin l(<strong><em>w</em></strong>)。刚刚我们求得，损失函数为
<img alt="图片6.png" src="assets/CgpVE1_lxUmAc_dEAAA5bSwYYTY904.png"/>
可见，损失函数是个关于 <strong><em>x</em></strong>i、yi 和 <strong><em>w</em></strong> 的函数，而<strong><em>x</em></strong>i 和 yi 是输入数据集中已知的条件，所以损失函数的未知数只有 <strong><em>w</em></strong>。</p>
<p>于是可以得到结论，逻辑回归最后一步的建模公式，实质上就是求解函数极值的问题。</p>
<blockquote>
<p>关于求极值，我们在《05 | 求极值：如何找到复杂业务的最优解？》曾详细介绍过求导法和梯度下降法。</p>
</blockquote>
<p>在这里，由于损失函数包含了非线性的 sigmoid 函数，求导法是无法得到解析解的；因此，我们使用梯度下降法来求解参数<strong><em>w</em></strong>。
<img alt="图片7.png" src="assets/Cip5yF_lxYeAZOMOAADJ2CJBEHc898.png"/>
<img alt="图片8.png" src="assets/CgqCHl_lxb2ALAYhAACvGE2H5Rw210.png"/>
我们已经计算出了损失函数关于模型参数的导数，这也是损失函数的梯度方向，我们可以利用先前所学的梯度下降法来求解函数的极值。</p>
<p>然而，这里存在一个计算效率的缺陷，即梯度函数中包含了大型求和的运算。这里的大型求和是 i 从 1 到 n 的计算，也就是对于整个数据集全部的数据去进行的全量计算。</p>
<blockquote>
<p>可以想象出，当输入的数据量非常大的时候，梯度下降法每次的迭代都会产生大量的计算。这样，建模过程中会消耗大量计算资源，模型更新效率也会受到很大影响。</p>
</blockquote>
<h4 id="随机梯度下降法">【随机梯度下降法】</h4>
<p>为了解决这个问题，人工智能领域常常用<strong>随机梯度下降法</strong>来修正<strong>梯度下降法</strong>的不足。随机梯度下降法与梯度下降法的区别只有一点，那就是随机梯度下降在每轮更新参数时，只随机选取一个样本 dm 来计算梯度，而非计算整个数据集梯度。其余的计算过程，二者完全一致。</p>
<p><img alt="图片9.png" src="assets/CgqCHl_lxdaAQ749AADJ2O9gnkU384.png"/></p>
<p>根据上面更新公式的算法，我们通过多轮迭代，就能最终求解出让 l(w) 取得最大值的参数向量<strong><em>w</em></strong>。</p>
<h3 id="逻辑回归代码实现">逻辑回归代码实现</h3>
<p>接下来，我们在下面的数据集上，分别采用逻辑回归来建立分类模型。</p>
<p>第一个数据集如下，其中每一行是一个样本，每一列一个特征，最后一列是样本的类别。</p>
<p><img alt="图片10.png" src="assets/Cip5yF_lxgCAcT3PAADKvRlRWh8558.png"/></p>
<p>第二个数据集如下，格式与第一个数据集相同。</p>
<p><img alt="图片11.png" src="assets/Cip5yF_lxgaAbVaDAADgP88xevs672.png"/></p>
<p>我们采用下面的代码，建立逻辑回归模型。</p>
<pre><code>import math

import numpy as np

import random

x = np.array([[1,1,1],[0,0,1],[0,1,1],[1,0,1]])

y = np.array([1,0,0,0])

#y = np.array([0,0,1,1])

w = np.array([0.5,0.5,0.5])

a = 0.01

maxloop = 10000

for _ in range(maxloop):

	m = random.randint(0,3)

	fi = 1.0/(1+math.pow(math.e,-np.dot(x[m],w)))

	g = (y[m] - fi)*x[m]

	w = w + a*g

	print w
</code></pre>
<p>【我们对代码进行走读】</p>
<ul>
<li>代码中，第 5～7 行分别输入数据集 x 和 y；</li>
<li>第 9 行，初始化参数向量，在这里，我们采用固定的初始化方法，你也可以调整为随机初始化；</li>
<li>第 11 行，设置学习率为 0.01；</li>
<li>第 12 行，设置最大迭代轮数为 10000 次。</li>
</ul>
<p>接下来进入随机梯度下降法的循环体。</p>
<ul>
<li>第 14 行，从 0 到 3 中随机抽取一个数字作为本轮迭代梯度的样本；</li>
<li>第 15 行，计算 Φ(zm)；</li>
<li>第 16 行，计算样本 m 带来的梯度 g；</li>
<li>第 17 行，利用随机梯度下降法更新参数 w；</li>
<li>第 18 行，打印这一轮的结果。</li>
</ul>
<h4 id="数据集一">【数据集一】</h4>
<p>运行上述代码，我们对数据集一建模得到的最优参数为 [3.1,3.0,-4.8]。利用这组参数，我们可以对数据集一的学习效果进行测试，如下表所示</p>
<p><img alt="图片12.png" src="assets/CgqCHl_lxhOAFnR4AAIK9izy6Cs182.png"/></p>
<p>可见，数据集一上，我们的模型全部正确预测，效果非常好。</p>
<h4 id="数据集二">【数据集二】</h4>
<p>再运行上述代码，我们对数据集二建模得到的最优参数为 [0.16, 0.10, -0.03]。利用这组参数，我们可以对数据集二的学习效果进行测试，如下表所示。
<img alt="图片13.png" src="assets/Ciqc1F_lxh2AB8_FAAIMnwsJ144124.png"/>
我们发现，在数据集二上，模型的预测结果只能是马马虎虎，这体现在两点：</p>
<ul>
<li>4 个样本中，并没有全部正确预测，样本 1 预测错误；</li>
<li>对于正确预测的 3 个样本而言，预测值都在边界线 0.5 附近，就算是正确预测，也没有压倒性优势。</li>
</ul>
<p>那么为什么同样的模型，只是换了数据集，效果就千差万别呢？</p>
<h3 id="逻辑回归的不足">逻辑回归的不足</h3>
<p>这是因为，逻辑回归是个线性模型，它只能处理线性问题。</p>
<p>例如，对于一个二维平面来说，线性模型就是一条直线。如果数据的分布不支持用一条线来分割的话，逻辑回归就无法收敛，如下图所示。
<img alt="图片3.png" src="assets/CgqCHl_lximANu7DAAC6Cmxv9rM944.png"/>
图中蓝色点是一类，黄色点是一类。现在，我们要用逻辑回归这样的线性模型来进行区分。可见，不论这条线怎么选，都是无法将两类样本进行分割的，这也是逻辑回归模型的缺陷。</p>
<blockquote>
<p>要想解决的话，只有用更复杂的模型，例如我们后续的课时中会介绍的决策树、神经网络等模型。</p>
</blockquote>
<h4 id="逻辑回归与线性回归">【逻辑回归与线性回归】</h4>
<p>在上一讲《18 | AI 入门：利用 3 个公式搭建最简 AI 框架》中，通过“身高预测”，我们从人工智能模型的视角，重新认识了线性回归，那么逻辑回归和线性回归的不同有哪些呢？</p>
<ul>
<li><strong>从名字上比较</strong></li>
</ul>
<p>线性回归是回归模型，是用一根“线”去回归出输入和输出之间的关系，即用一根线去尽可能把全部样本“串”起来。</p>
<p>而逻辑回归虽然名字里有“回归”二字，但其实是一个分类模型，它是希望用一根线去把两波样本尽可能分开。</p>
<ul>
<li><strong>从表达式上看</strong></li>
</ul>
<p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数的映射，在最终的类别判断上，还需要对比一下预测值和 0.5 之间的大小关系。</p>
<p>因此，线性回归解决的是回归问题，输出的连续值；而逻辑回归解决的是二分类问题，输出的是“0”或“1”的离散值。</p>
<ul>
<li><strong>从机理上看</strong></li>
</ul>
<p>逻辑回归增加了 sigmoid 函数，可以让预测结果在 0.5 附近产生更大的变化率，变化率更大，意味着梯度更大。</p>
<p>在使用梯度下降法的时候，这样的机理，让模型的预测值会倾向于离开变化率大的地方，而收敛在“0”或“1”附近。这样的模型机理，会让它更适合用于分类问题的建模，具有更好的鲁棒性。</p>
<h3 id="小结">小结</h3>
<p><strong>逻辑回归</strong>是人工智能领域中分类问题的入门级算法。利用 AI 基本框架来看，它的 3 个核心公式分别是
<img alt="WechatIMG1406.png" src="assets/Ciqc1F_pRgWAY4ynAABjhuo5U0E756.png"/>
逻辑回归是个线性模型，具有计算简单、可解释性强等优势。它的不足是，只能处理线性问题，对于非线性问题则束手无策。</p>
<p>最后，我们留一个思考题。试着把本课时中的代码，由随机梯度下降法改写为梯度下降法，再来求解一次参数 <strong><em>w</em></strong> 吧。原则上除了计算量会变大以外，对分类结果是不会产生改变的。不妨亲自试一下。</p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#69050505505d5858595e290e04080005470a0604" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'93593b6bdbf08024',t:'MTc0NTUzNTc4Ni4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>