<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="18 从全局到局部：核技巧" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>18 从全局到局部：核技巧 </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%e6%89%93%e9%80%9a%e4%bf%ae%e7%82%bc%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%bb%bb%e7%9d%a3%e4%ba%8c%e8%84%89.md.html" id="00 开篇词 打通修炼机器学习的任督二脉.md.html">00 开篇词 打通修炼机器学习的任督二脉.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/01%20%e9%a2%91%e7%8e%87%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="01 频率视角下的机器学习.md.html">01 频率视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/02%20%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="02 贝叶斯视角下的机器学习.md.html">02 贝叶斯视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/03%20%e5%ad%a6%e4%bb%80%e4%b9%88%e4%b8%8e%e6%80%8e%e4%b9%88%e5%ad%a6.md.html" id="03 学什么与怎么学.md.html">03 学什么与怎么学.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/04%20%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba.md.html" id="04 计算学习理论.md.html">04 计算学习理论.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/05%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%88%86%e7%b1%bb%e6%96%b9%e5%bc%8f.md.html" id="05 模型的分类方式.md.html">05 模型的分类方式.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/06%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%be%e8%ae%a1%e5%87%86%e5%88%99.md.html" id="06 模型的设计准则.md.html">06 模型的设计准则.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/07%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%aa%8c%e8%af%81%e6%96%b9%e6%b3%95.md.html" id="07 模型的验证方法.md.html">07 模型的验证方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/08%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87.md.html" id="08 模型的评估指标.md.html">08 模型的评估指标.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/09%20%e5%ae%9e%e9%aa%8c%e8%ae%be%e8%ae%a1.md.html" id="09 实验设计.md.html">09 实验设计.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/10%20%e7%89%b9%e5%be%81%e9%a2%84%e5%a4%84%e7%90%86.md.html" id="10 特征预处理.md.html">10 特征预处理.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/11%20%e5%9f%ba%e7%a1%80%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%9a%e4%b8%80%e5%85%83%e4%b8%8e%e5%a4%9a%e5%85%83.md.html" id="11 基础线性回归：一元与多元.md.html">11 基础线性回归：一元与多元.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/12%20%e6%ad%a3%e5%88%99%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9a%e6%94%b6%e7%bc%a9%e6%96%b9%e6%b3%95%e4%b8%8e%e8%be%b9%e9%99%85%e5%8c%96.md.html" id="12 正则化处理：收缩方法与边际化.md.html">12 正则化处理：收缩方法与边际化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/13%20%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e4%b8%bb%e6%88%90%e5%88%86%e7%9a%84%e4%bd%bf%e7%94%a8.md.html" id="13 线性降维：主成分的使用.md.html">13 线性降维：主成分的使用.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/14%20%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0.md.html" id="14 非线性降维：流形学习.md.html">14 非线性降维：流形学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/15%20%e4%bb%8e%e5%9b%9e%e5%bd%92%e5%88%b0%e5%88%86%e7%b1%bb%ef%bc%9a%e8%81%94%e7%b3%bb%e5%87%bd%e6%95%b0%e4%b8%8e%e9%99%8d%e7%bb%b4.md.html" id="15 从回归到分类：联系函数与降维.md.html">15 从回归到分类：联系函数与降维.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/16%20%e5%bb%ba%e6%a8%a1%e9%9d%9e%e6%ad%a3%e6%80%81%e5%88%86%e5%b8%83%ef%bc%9a%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b.md.html" id="16 建模非正态分布：广义线性模型.md.html">16 建模非正态分布：广义线性模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/17%20%e5%87%a0%e4%bd%95%e8%a7%92%e5%ba%a6%e7%9c%8b%e5%88%86%e7%b1%bb%ef%bc%9a%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba.md.html" id="17 几何角度看分类：支持向量机.md.html">17 几何角度看分类：支持向量机.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/18%20%e4%bb%8e%e5%85%a8%e5%b1%80%e5%88%b0%e5%b1%80%e9%83%a8%ef%bc%9a%e6%a0%b8%e6%8a%80%e5%b7%a7.md.html" id="18 从全局到局部：核技巧.md.html">18 从全局到局部：核技巧.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/19%20%e9%9d%9e%e5%8f%82%e6%95%b0%e5%8c%96%e7%9a%84%e5%b1%80%e9%83%a8%e6%a8%a1%e5%9e%8b%ef%bc%9aK%e8%bf%91%e9%82%bb.md.html" id="19 非参数化的局部模型：K近邻.md.html">19 非参数化的局部模型：K近邻.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/20%20%e5%9f%ba%e4%ba%8e%e8%b7%9d%e7%a6%bb%e7%9a%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e8%81%9a%e7%b1%bb%e4%b8%8e%e5%ba%a6%e9%87%8f%e5%ad%a6%e4%b9%a0.md.html" id="20 基于距离的学习：聚类与度量学习.md.html">20 基于距离的学习：聚类与度量学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/21%20%e5%9f%ba%e5%87%bd%e6%95%b0%e6%89%a9%e5%b1%95%ef%bc%9a%e5%b1%9e%e6%80%a7%e7%9a%84%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%8c%96.md.html" id="21 基函数扩展：属性的非线性化.md.html">21 基函数扩展：属性的非线性化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/22%20%e8%87%aa%e9%80%82%e5%ba%94%e7%9a%84%e5%9f%ba%e5%87%bd%e6%95%b0%ef%bc%9a%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c.md.html" id="22 自适应的基函数：神经网络.md.html">22 自适应的基函数：神经网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/23%20%e5%b1%82%e6%ac%a1%e5%8c%96%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%ef%bc%9a%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0.md.html" id="23 层次化的神经网络：深度学习.md.html">23 层次化的神经网络：深度学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/24%20%e6%b7%b1%e5%ba%a6%e7%bc%96%e8%a7%a3%e7%a0%81%ef%bc%9a%e8%a1%a8%e7%a4%ba%e5%ad%a6%e4%b9%a0.md.html" id="24 深度编解码：表示学习.md.html">24 深度编解码：表示学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/25%20%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e5%8c%ba%e5%9f%9f%e5%88%92%e5%88%86%ef%bc%9a%e6%a0%91%e6%a8%a1%e5%9e%8b.md.html" id="25 基于特征的区域划分：树模型.md.html">25 基于特征的区域划分：树模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/26%20%e9%9b%86%e6%88%90%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9aBoosting%e4%b8%8eBagging.md.html" id="26 集成化处理：Boosting与Bagging.md.html">26 集成化处理：Boosting与Bagging.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/27%20%e4%b8%87%e8%83%bd%e6%a8%a1%e5%9e%8b%ef%bc%9a%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97.md.html" id="27 万能模型：梯度提升与随机森林.md.html">27 万能模型：梯度提升与随机森林.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/28%20%e6%9c%80%e7%ae%80%e5%8d%95%e7%9a%84%e6%a6%82%e7%8e%87%e5%9b%be%ef%bc%9a%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="28 最简单的概率图：朴素贝叶斯.md.html">28 最简单的概率图：朴素贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/29%20%e6%9c%89%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="29 有向图模型：贝叶斯网络.md.html">29 有向图模型：贝叶斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/30%20%e6%97%a0%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%9a%8f%e6%9c%ba%e5%9c%ba.md.html" id="30 无向图模型：马尔可夫随机场.md.html">30 无向图模型：马尔可夫随机场.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/31%20%e5%bb%ba%e6%a8%a1%e8%bf%9e%e7%bb%ad%e5%88%86%e5%b8%83%ef%bc%9a%e9%ab%98%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="31 建模连续分布：高斯网络.md.html">31 建模连续分布：高斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/32%20%e4%bb%8e%e6%9c%89%e9%99%90%e5%88%b0%e6%97%a0%e9%99%90%ef%bc%9a%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b.md.html" id="32 从有限到无限：高斯过程.md.html">32 从有限到无限：高斯过程.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/33%20%e5%ba%8f%e5%88%97%e5%8c%96%e5%bb%ba%e6%a8%a1%ef%bc%9a%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b.md.html" id="33 序列化建模：隐马尔可夫模型.md.html">33 序列化建模：隐马尔可夫模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/34%20%e8%bf%9e%e7%bb%ad%e5%ba%8f%e5%88%97%e5%8c%96%e6%a8%a1%e5%9e%8b%ef%bc%9a%e7%ba%bf%e6%80%a7%e5%8a%a8%e6%80%81%e7%b3%bb%e7%bb%9f.md.html" id="34 连续序列化模型：线性动态系统.md.html">34 连续序列化模型：线性动态系统.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/35%20%e7%b2%be%e7%a1%ae%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e9%87%8f%e6%b6%88%e9%99%a4%e5%8f%8a%e5%85%b6%e6%8b%93%e5%b1%95.md.html" id="35 精确推断：变量消除及其拓展.md.html">35 精确推断：变量消除及其拓展.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/36%20%e7%a1%ae%e5%ae%9a%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e5%88%86%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="36 确定近似推断：变分贝叶斯.md.html">36 确定近似推断：变分贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/37%20%e9%9a%8f%e6%9c%ba%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9aMCMC.md.html" id="37 随机近似推断：MCMC.md.html">37 随机近似推断：MCMC.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/38%20%e5%ae%8c%e5%a4%87%e6%95%b0%e6%8d%ae%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9a%e6%9c%89%e5%90%91%e5%9b%be%e4%b8%8e%e6%97%a0%e5%90%91%e5%9b%be.md.html" id="38 完备数据下的参数学习：有向图与无向图.md.html">38 完备数据下的参数学习：有向图与无向图.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/39%20%e9%9a%90%e5%8f%98%e9%87%8f%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9aEM%e6%96%b9%e6%b3%95%e4%b8%8e%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b.md.html" id="39 隐变量下的参数学习：EM方法与混合模型.md.html">39 隐变量下的参数学习：EM方法与混合模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/40%20%e7%bb%93%e6%9e%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e5%9f%ba%e4%ba%8e%e7%ba%a6%e6%9d%9f%e4%b8%8e%e5%9f%ba%e4%ba%8e%e8%af%84%e5%88%86.md.html" id="40 结构学习：基于约束与基于评分.md.html">40 结构学习：基于约束与基于评分.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e5%a6%82%e4%bd%95%e6%88%90%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%b7%a5%e7%a8%8b%e5%b8%88%ef%bc%9f.md.html" id="如何成为机器学习工程师？.md.html">如何成为机器学习工程师？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 机器学习的模型体系.md.html">总结课 机器学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 贝叶斯学习的模型体系.md.html">总结课 贝叶斯学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e7%bb%93%e8%af%be%20%e7%bb%88%e6%9c%89%e4%b8%80%e5%a4%a9%ef%bc%8c%e4%bd%a0%e5%b0%86%e4%b8%ba%e4%bb%8a%e5%a4%a9%e7%9a%84%e4%bb%98%e5%87%ba%e9%aa%84%e5%82%b2.md.html" id="结课 终有一天，你将为今天的付出骄傲.md.html">结课 终有一天，你将为今天的付出骄傲.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="18 从全局到局部：核技巧" id="title">18 从全局到局部：核技巧</h1>
<div><p>俗话说得好：“支持向量机有三宝，间隔对偶核技巧”。在上一篇文章中我和你分享了间隔这个核心概念，今天就来看看对偶和核技巧的使用。对偶性主要应用在最优决策边界的求解中，其逻辑比较简单。</p>
<p>但在介绍核技巧时，会先后涉及核函数、核方法、核技巧这些近似的概念。虽然从名字上看，它们都是“核”字辈的兄弟，但是在含义和用途上却不能一概而论，因此有必要对它们做一些系统的梳理。</p>
<p>当支持向量机用于线性可分的数据时，不同类别的支持向量到最优决策边界的距离之和为<span class="math inline">\(2 / || {\\bf w} ||\)</span>，其中的<span class="math inline">\({\\bf w}\)</span>是超平面的线性系数，也就是法向量。不难看出，让间隔<span class="math inline">\(|| {\\bf w} || ^ {-1}\)</span>最大化就是让<span class="math inline">\(|| {\\bf w} || ^ 2\)</span>最小化，所以线性可分的支持向量机对应的最优化问题就是</p>
<p><span class="math display">\[ \\mathop {\\min }\\limits_{{\\bf w}, b} \\dfrac{1}{2} || {\\bf w} || ^ 2\]</span></p><p><span class="math display">\[ {\\rm s.t.} y_i ({\\bf w} \\cdot {\\bf x}_i + b) \\ge 1\]</span></p><p>其中<span class="math inline">\(y_i\)</span>为数据点<span class="math inline">\({\\bf x}_i\)</span>对应的类别，其取值为<span class="math inline">\(\\pm 1\)</span>。</p>
<p>这个问题本身是个<strong>凸二次规划</strong>（convex quadratic programming）问题，求解起来轻松加随意。但借助拉格朗日乘子，这个原问题（primal problem）就可以改写成所谓的广义拉格朗日函数（generalized Lagrange function）</p>
<p><span class="math display">\[ L({\\bf w}, b, {\\boldsymbol \\alpha}) = \\dfrac{1}{2} || {\\bf w} || ^ 2 + \\sum\\limits_{i = 1}^m \\alpha_i\[1 - y_i ({\\bf w} \\cdot {\\bf x}_i + b)\] \]</span></p><p>其中每个<span class="math inline">\(\\alpha_i\)</span>都是<span class="math inline">\(\\boldsymbol \\alpha\)</span>的分量。和原来的优化问题相比，除了和决策边界有关的变量<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>之外，广义拉格朗日函数还引入了一组不小于0的参数<span class="math inline">\(\\alpha_i\)</span>。</p>
<p>这个式子其实从另一个角度说明了为什么最优决策边界只取决于几个支持向量。对于不是支持向量的数据点来说，等式右侧第二项中的<span class="math inline">\(1 - y_i ({\\bf w} \\cdot {\\bf x}_i + b)\)</span>是小于0的，因此在让<span class="math inline">\(L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>最小化时，必须把这些点的贡献去除，去除的方式就是让系数<span class="math inline">\(\\alpha_i = 0\)</span>。这样一来，它们就成来支持向量机里的路人甲乙丙丁了。</p>
<p>广义拉格朗日函数的最优化可以分成两步：先把<span class="math inline">\( L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>看成<span class="math inline">\(\\boldsymbol \\alpha\)</span>的函数，在将<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>视为常量的前提下求出其最大值。由于<span class="math inline">\(\\boldsymbol \\alpha\)</span>在求最值时被消去，这时求解出的</p>
<p><span class="math display">\[\\theta _p ({\\bf w}, b) = \\mathop {\\max}\\limits_{\\boldsymbol \\alpha} L({\\bf w}, b, {\\boldsymbol \\alpha})\]</span></p><p>就是只和<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>有关的函数了。</p>
<p>接下来如何确定最优的决策边界参数呢？这里要分两种情况来考虑。当参数<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>不满足原问题的约束时，总会找到能让目标函数取值为正无穷的<span class="math inline">\(\\boldsymbol \\alpha\)</span>，这意味着最大值其实就是不存在。只有符合原问题的要求时，<span class="math inline">\(\\theta _P ({\\bf w}, b)\)</span>的最大值才有意义。</p>
<p>那么这个最大值等于多少呢？由于<span class="math inline">\(\\alpha_i\)</span>和<span class="math inline">\(1 - y_i ({\\bf w} \\cdot {\\bf x}_i + b)\)</span>的符号相反，因此两者之积必然是小于0的，由此不难得出<span class="math inline">\(\\theta _P ({\\bf w}, b) = || {\\bf w} || ^ 2 / 2\)</span>。这里需要注意的是，在确定最优的<span class="math inline">\(\\boldsymbol \\alpha\)</span>时，原始的优化对象<span class="math inline">\(|| {\\bf w} || ^ 2 / 2\)</span>是作为常数项出现的。</p>
<p>经过一番折腾之后，原始的最小化问题就被等效为<span class="math inline">\(\\mathop {\\min }\\limits_{{\\bf w}, b} \\theta _p ({\\bf w}, b)\)</span>，也就是广义拉格朗日函数的极小极大问题。这个极小极大问题是先对<span class="math inline">\(\\boldsymbol \\alpha\)</span>求极大，再对<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>求极小。如果对上一季的内容还有印象的话，你是不是会一拍大腿：一边最大，一边最小，这不就是传说中的鞍点（saddle point）嘛！</p>
<p>计算多重积分时，调换积分顺序是简化计算的常用技巧，这种思路在对偶问题中同样大有用武之地。“极小极大”调个个儿就变成了“极大极小”，确定参数的顺序也要相应地反转。对于支持向量机来说，其广义拉格朗日函数的极大极小问题具有如下的形式</p>
<p><span class="math display">\[ \\mathop {\\max}\\limits_{\\boldsymbol \\alpha} \\theta _D(\\boldsymbol \\alpha) = \\mathop {\\max}\\limits_{\\boldsymbol \\alpha} \\mathop {\\min }\\limits_{{\\bf w}, b} L({\\bf w}, b, {\\boldsymbol \\alpha}) \]</span></p><p>让广义拉格朗日函数对决策边界的两个参数<span class="math inline">\(\\bf w\)</span>和<span class="math inline">\(b\)</span>的偏导数为0，就可以得到</p>
<p><span class="math display">\[ {\\bf w} = \\sum\\limits_{i = 1}^m \\alpha_i y_i {\\bf x}_i, \\sum\\limits_{i = 1}^m \\alpha_i y_i = 0 \]</span></p><p>将解出的约束关系先代入到<span class="math inline">\(L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>中，再作为拉格朗日乘子项引入<span class="math inline">\(L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>的优化，就可以得到原优化问题的对偶问题（dual problem）</p>
<p><span class="math display">\[ \\mathop {\\max}\\limits_{\\boldsymbol \\alpha} \\sum\\limits_{i = 1}^m \\alpha_i - \\dfrac{1}{2} \\sum\\limits_{i = 1}^m \\sum\\limits_{j = 1}^m \\alpha_i \\alpha_j y_i y_j {\\bf x}_i^T {\\bf x}_j\]</span></p><p><span class="math display">\[ {\\rm s.t.} \\sum\\limits_{i = 1}^m \\alpha_i y_i = 0, \\alpha_i \\ge 0\]</span></p><p>虽然一顿操作猛如虎将原问题变成了对偶问题，但这两者之间到底能不能完全划等号还是个未知数呢。直观地看，原函数求出来的是<span class="math inline">\(L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>最大值的下界，对偶函数求出来的是<span class="math inline">\(L({\\bf w}, b, {\\boldsymbol \\alpha})\)</span>最小值的上界，后者肯定不会大于前者，但也不是无条件地相等。</p>
<p>好在在数学上可以证明，当上面的过程满足<strong>Karush-Kuhn-Tucker条件</strong>（简称KKT条件，是一组关于<span class="math inline">\(\\boldsymbol \\alpha\)</span>、<span class="math inline">\({\\bf w}\)</span>和<span class="math inline">\(b\)</span>的不等式）时，原问题和对偶问题才能殊途同归。<strong>支持向量机对原问题和对偶问题之间等价关系的利用就是它的对偶性（duality）</strong>。</p>
<p>说完了对偶性，下面就轮到核技巧了。在核技巧这台大戏里，第一个出场的是<strong>核函数</strong>，这才是“核”字辈这些兄弟里的开山鼻祖。</p>
<p>要理解核函数，还是要从史上最著名的线性不可分问题——异或问题出发。假设待分类的四个点<span class="math inline">\((x_1, x_2)\)</span>分别为<span class="math inline">\((\\pm 0, \\pm 1)\)</span>，那么只需要添加一个多项式形式的新属性<span class="math inline">\(\\phi ({\\bf x}) = (x_1 - x_2) ^ 2\)</span>，就可以将原来的四个点分别映射为三维空间上的(0, 0, 0), (0, 1, 1), (1, 0, 1)和(1, 1, 0)。这时，在三维空间中只需要将原来的数据平面稍微向上抬一点，就能完美地区分两个类别了。</p>
<p>既然<span class="math inline">\(\\phi ({\\bf x})\)</span>能生成新的属性，它就是传说中的核函数吧？非也！<span class="math inline">\(\\phi ({\\bf x})\)</span>只是特征映射（feature map），它的作用是从原始属性生成新的特征。<strong>对高维空间上新生成的特征向量进行内积运算，得到的才是真正的核函数</strong>（kernel function）。核函数的数学表达式具有如下的形式</p>
<p><span class="math display">\[ k({\\bf x}, {\\bf x}’) = \\phi ({\\bf x}) ^ T \\phi ({\\bf x}’) \]</span></p><p>核函数的这个公式给出了生成条件而非判定条件。当给定特征的映射方式后，可以用它来计算核函数；但是当给出一个确定的函数时，如何判定它能不能作为核函数呢？<strong>梅塞尔定理</strong>（Mercer’s theorem）解决了这个判定问题。</p>
<p>这个定理于1909年由英国数学家詹姆斯·梅塞尔（James Mercer）提出，其内容是任何满足对称性和半正定性的函数都是某个高维希尔伯特空间的内积。只要一个函数满足这两个条件，它就可以用做核函数。但梅塞尔定理只是判定核函数的充分而非必要条件，不满足梅塞尔定理的函数也可能是核函数。</p>
<p>之所以要将特征映射表示成核函数，是因为内积的引入简化了高维空间中的复杂运算。映射到高维空间后，待优化的对偶问题就变成了</p>
<p><span class="math display">\[ \\mathop {\\max}\\limits_{\\boldsymbol \\alpha} \\sum\\limits_{i = 1}^m \\alpha_i - \\dfrac{1}{2} \\sum\\limits_{i = 1}^m \\sum\\limits_{j = 1}^m \\alpha_i \\alpha_j y_i y_j \\phi({\\bf x}_i)^T \\phi({\\bf x}_j) \]</span></p><p>按照一般的思路，要直接计算上面的表达式就先得写出<span class="math inline">\(\\phi (\\cdot)\)</span>的形式，再在新的高维特征空间上计算内积，但这在实际运算中存在很大困难。尤其是当<span class="math inline">\(\\phi (\\cdot)\)</span>的表达式未知时，那这内积就没法计算了。可即使<span class="math inline">\(\\phi (\\cdot)\)</span>的形式已知，如果特征空间的维数较高，甚至达到无穷维的话，内积的运算也会非常困难。</p>
<p>这时就需要核函数来发挥威力了。核函数说到底是瓦普尼克“能走直线就别兜圈子”思想的产物。既然优化的对象是内积的结果，那么直接定义内积的表达式就可以了，何苦还要引入特征映射和特征空间这些个中间步骤呢？更重要的是，梅塞尔定理为这种捷径提供了理论依据，只要核函数满足对称性和半正定的条件，对应的映射空间就铁定存在。</p>
<p>所以核函数的引入相当于隐式定义了特征映射和特征空间，无需关心这些中间结果的形式就能直接计算待优化的内积，从而大大简化计算。</p>
<p>从核函数出发，可以衍生出其他和“核”相关的概念。<strong>从思想上讲，核方法（kernel method）表示的是将低维空间中的线性不可分问题通常可以转化为高维空间中的线性可分问题的思路；从运算上讲，核技巧（kernel trick）表示的是通过间接定义特征映射来直接计算内积的运算方法</strong>。两者就像同一枚硬币的两面，虽然浑然一体但还是有所区别，因而有必要加以说明。</p>
<p>在实际应用中，有一类特殊的<strong>平稳核函数</strong>（stationary kernel），它的参数是两个原始参数之差，也就是<span class="math inline">\(k({\\bf x}, {\\bf x}’) = k({\\bf x} - {\\bf x}’)\)</span>。平稳核函数满足平移不变性（translation invariance），只要输入<span class="math inline">\(\\bf x\)</span>和<span class="math inline">\({\\bf x}’\)</span>的相对位置不变，核函数的取值就不会发生变化。如果在平移不变性的基础上再定义<strong>各向同性</strong>（homogeneity），那核函数的取值就会进一步与方向无关，这样的核函数就可以表示为<span class="math inline">\(k({\\bf x}, {\\bf x}’) = k(|| {\\bf x} - {\\bf x}’ ||)\)</span>。</p>
<p>一种满足平移不变性和各向同性的核函数是<strong>径向基核</strong>（radial basis function kernel），其表达式为</p>
<p><span class="math display">\[ k({\\bf x}, {\\bf x}’) = \\exp(-\\dfrac{|| {\\bf x} - {\\bf x}’ || ^ 2}{2\\sigma ^ 2}) \]</span></p><p>在数学上可以推导出，径向基核所对应的特征映射是无穷维的，也就是隐式的特征空间是无穷维的空间。计算无穷维的特征映射是个复杂的任务，但径向基核的出现聪明地绕开了这个障碍。应用在支持向量机中，径向基核可以将线性边界变换成非线性边界。</p>
<p>在Scikit-learn中设置核函数的方法并不难，只需要将SVC类中的参数kernel设置为’rbf’即可（也可以使用其他类型的核函数）。径向基核的参数<span class="math inline">\(\\sigma\)</span>决定了高斯函数的宽度，但在SVC类中，这个参数是以<span class="math inline">\(\\gamma = 1 / 2\\sigma ^ 2\)</span>的形式出现的，这意味着调用SVC类时<span class="math inline">\(\\gamma\)</span>设置得越大，核的宽度实际上就越窄。</p>
<p>除了核宽度之外，另一个需要需要设置的是正则化参数<span class="math inline">\(C\)</span>，这个参数越大，正则化的效果就越弱，当<span class="math inline">\(C\)</span>接近正无穷时，计算出来的就是未经正则化处理的结果。将径向基核应用到线性不可分的数据集中，就可以将两类数据完全分开，如下图所示。在结果中，较大的<span class="math inline">\(C\)</span>让最优决策边界有过拟合的趋势。</p>
<p><img alt="" src="assets/66d924eaf06752e0230d0ef1f6c105e2.png"/></p>
<p>使用径向基核的支持向量机对曼城-西布朗数据集的分类结果</p>
<p><strong>除了简化内积运算之外，核函数更本质的意义在于对相似性度量（similarity measure）的表示</strong>。回忆一下线性代数的内容，内积表示的是两个向量之间的关系。如果将两个向量归一化后再来计算内积，那么求出来的就是两者之间的夹角。而作为原始内积的非线性拓展，核函数重新定义了数据的表征框架：将每个维度上的绝对坐标替换成两两之间的相似度。</p>
<p>这样一来，分类问题就变成了从几何意义出发，基于相似性度量在高维的特征空间上找到线性决策边界，再将它映射成低维空间上非线性的决策边界。</p>
<p>在直观的认识中，两个数据点相距越近，它们归属于同一类别的可能性就越高。如果将径向基的结果看成数据点相似度的话，那么<span class="math inline">\(\\bf x\)</span>和<span class="math inline">\({\\bf x}’\)</span>离得越近，两者之间的相似度就越高（接近于1）；反过来离得越远，相似度就越低（接近于0）。</p>
<p>接下来，计算出的相似度就成为分类的依据：和哪个类别的相似度高，未知的数据点就归属于哪个类别。和线性判别分析和逻辑回归这些参数化的分类模型相比，核函数更多地借鉴了物以类聚的简单逻辑。</p>
<p>将这种逻辑引申一步就可以得到，<strong>核函数是实现局部化（localization）的工具</strong>。在解决回归问题时，核函数本质上也是一组权重系数，但它和线性模型中权重系数的区别在于它是取决于距离的，由距离表征的相似度决定了系数的取值。在整体上，数据空间的全局参数并不能通过最小二乘等全局性方法计算出来，而是要将每个核函数所表示的局部尺度特征叠加在一起。</p>
<p>这样看来，每个核函数都像是战国中雄踞一方的诸侯，其势力在远离权力中心的过程中不断减弱。和这些叱咤一方的诸侯相比，作为全局参数模型的周天子就完全是个摆设了。</p>
<p>今天我和你分享了支持向量机中对偶和核技巧的概念与原理，其要点如下：</p>
<ul>
<li><p>支持向量机在求解最优边界时需要利用对偶性，将原问题转化为对偶问题求解；</p></li>
<li><p>在思想上，核方法将高维空间上的线性边界转化成低维空间上的非线性边界；</p></li>
<li><p>在运算上，核技巧能在低维空间中直接计算高维空间中的内积；</p></li>
<li><p>核函数具有局部化的特点，是从全局模型到局部模型的过渡手段。</p></li>
</ul>
<p>其实在“人工智能基础课中”，高斯形式的径向基函数就有过亮相，它出现在径向基神经网络的介绍中。你可以复习一下这一部分的内容，借此加深对核函数与局部特性关系的理解。</p>
<p><strong>拓展阅读</strong></p>
<p>《人工神经网络 | 各人自扫门前雪：径向基函数神经网络》</p>
<p>说明：知识具有内在联系性，有些内容在“人工智能基础课”里有不同角度的介绍。拓展阅读是为了让你更方便地回顾内容，如已订阅可以直接点击进入文章复习。不阅读，也不影响当前的学习。</p>
<p><img alt="" src="assets/28e766563bb973126e37b6266b402aa7.jpg"/></p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#066a6a6a3f323737363146616b676f6a2865696b" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9359dddb6f623b08',t:'MTc0NTU0MjQ0MC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>