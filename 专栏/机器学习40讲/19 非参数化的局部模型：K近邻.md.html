<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="19 非参数化的局部模型：K近邻" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>19 非参数化的局部模型：K近邻 </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%e6%89%93%e9%80%9a%e4%bf%ae%e7%82%bc%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%bb%bb%e7%9d%a3%e4%ba%8c%e8%84%89.md.html" id="00 开篇词 打通修炼机器学习的任督二脉.md.html">00 开篇词 打通修炼机器学习的任督二脉.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/01%20%e9%a2%91%e7%8e%87%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="01 频率视角下的机器学习.md.html">01 频率视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/02%20%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="02 贝叶斯视角下的机器学习.md.html">02 贝叶斯视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/03%20%e5%ad%a6%e4%bb%80%e4%b9%88%e4%b8%8e%e6%80%8e%e4%b9%88%e5%ad%a6.md.html" id="03 学什么与怎么学.md.html">03 学什么与怎么学.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/04%20%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba.md.html" id="04 计算学习理论.md.html">04 计算学习理论.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/05%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%88%86%e7%b1%bb%e6%96%b9%e5%bc%8f.md.html" id="05 模型的分类方式.md.html">05 模型的分类方式.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/06%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%be%e8%ae%a1%e5%87%86%e5%88%99.md.html" id="06 模型的设计准则.md.html">06 模型的设计准则.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/07%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%aa%8c%e8%af%81%e6%96%b9%e6%b3%95.md.html" id="07 模型的验证方法.md.html">07 模型的验证方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/08%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87.md.html" id="08 模型的评估指标.md.html">08 模型的评估指标.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/09%20%e5%ae%9e%e9%aa%8c%e8%ae%be%e8%ae%a1.md.html" id="09 实验设计.md.html">09 实验设计.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/10%20%e7%89%b9%e5%be%81%e9%a2%84%e5%a4%84%e7%90%86.md.html" id="10 特征预处理.md.html">10 特征预处理.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/11%20%e5%9f%ba%e7%a1%80%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%9a%e4%b8%80%e5%85%83%e4%b8%8e%e5%a4%9a%e5%85%83.md.html" id="11 基础线性回归：一元与多元.md.html">11 基础线性回归：一元与多元.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/12%20%e6%ad%a3%e5%88%99%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9a%e6%94%b6%e7%bc%a9%e6%96%b9%e6%b3%95%e4%b8%8e%e8%be%b9%e9%99%85%e5%8c%96.md.html" id="12 正则化处理：收缩方法与边际化.md.html">12 正则化处理：收缩方法与边际化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/13%20%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e4%b8%bb%e6%88%90%e5%88%86%e7%9a%84%e4%bd%bf%e7%94%a8.md.html" id="13 线性降维：主成分的使用.md.html">13 线性降维：主成分的使用.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/14%20%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0.md.html" id="14 非线性降维：流形学习.md.html">14 非线性降维：流形学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/15%20%e4%bb%8e%e5%9b%9e%e5%bd%92%e5%88%b0%e5%88%86%e7%b1%bb%ef%bc%9a%e8%81%94%e7%b3%bb%e5%87%bd%e6%95%b0%e4%b8%8e%e9%99%8d%e7%bb%b4.md.html" id="15 从回归到分类：联系函数与降维.md.html">15 从回归到分类：联系函数与降维.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/16%20%e5%bb%ba%e6%a8%a1%e9%9d%9e%e6%ad%a3%e6%80%81%e5%88%86%e5%b8%83%ef%bc%9a%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b.md.html" id="16 建模非正态分布：广义线性模型.md.html">16 建模非正态分布：广义线性模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/17%20%e5%87%a0%e4%bd%95%e8%a7%92%e5%ba%a6%e7%9c%8b%e5%88%86%e7%b1%bb%ef%bc%9a%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba.md.html" id="17 几何角度看分类：支持向量机.md.html">17 几何角度看分类：支持向量机.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/18%20%e4%bb%8e%e5%85%a8%e5%b1%80%e5%88%b0%e5%b1%80%e9%83%a8%ef%bc%9a%e6%a0%b8%e6%8a%80%e5%b7%a7.md.html" id="18 从全局到局部：核技巧.md.html">18 从全局到局部：核技巧.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/19%20%e9%9d%9e%e5%8f%82%e6%95%b0%e5%8c%96%e7%9a%84%e5%b1%80%e9%83%a8%e6%a8%a1%e5%9e%8b%ef%bc%9aK%e8%bf%91%e9%82%bb.md.html" id="19 非参数化的局部模型：K近邻.md.html">19 非参数化的局部模型：K近邻.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/20%20%e5%9f%ba%e4%ba%8e%e8%b7%9d%e7%a6%bb%e7%9a%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e8%81%9a%e7%b1%bb%e4%b8%8e%e5%ba%a6%e9%87%8f%e5%ad%a6%e4%b9%a0.md.html" id="20 基于距离的学习：聚类与度量学习.md.html">20 基于距离的学习：聚类与度量学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/21%20%e5%9f%ba%e5%87%bd%e6%95%b0%e6%89%a9%e5%b1%95%ef%bc%9a%e5%b1%9e%e6%80%a7%e7%9a%84%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%8c%96.md.html" id="21 基函数扩展：属性的非线性化.md.html">21 基函数扩展：属性的非线性化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/22%20%e8%87%aa%e9%80%82%e5%ba%94%e7%9a%84%e5%9f%ba%e5%87%bd%e6%95%b0%ef%bc%9a%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c.md.html" id="22 自适应的基函数：神经网络.md.html">22 自适应的基函数：神经网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/23%20%e5%b1%82%e6%ac%a1%e5%8c%96%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%ef%bc%9a%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0.md.html" id="23 层次化的神经网络：深度学习.md.html">23 层次化的神经网络：深度学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/24%20%e6%b7%b1%e5%ba%a6%e7%bc%96%e8%a7%a3%e7%a0%81%ef%bc%9a%e8%a1%a8%e7%a4%ba%e5%ad%a6%e4%b9%a0.md.html" id="24 深度编解码：表示学习.md.html">24 深度编解码：表示学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/25%20%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e5%8c%ba%e5%9f%9f%e5%88%92%e5%88%86%ef%bc%9a%e6%a0%91%e6%a8%a1%e5%9e%8b.md.html" id="25 基于特征的区域划分：树模型.md.html">25 基于特征的区域划分：树模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/26%20%e9%9b%86%e6%88%90%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9aBoosting%e4%b8%8eBagging.md.html" id="26 集成化处理：Boosting与Bagging.md.html">26 集成化处理：Boosting与Bagging.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/27%20%e4%b8%87%e8%83%bd%e6%a8%a1%e5%9e%8b%ef%bc%9a%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97.md.html" id="27 万能模型：梯度提升与随机森林.md.html">27 万能模型：梯度提升与随机森林.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/28%20%e6%9c%80%e7%ae%80%e5%8d%95%e7%9a%84%e6%a6%82%e7%8e%87%e5%9b%be%ef%bc%9a%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="28 最简单的概率图：朴素贝叶斯.md.html">28 最简单的概率图：朴素贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/29%20%e6%9c%89%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="29 有向图模型：贝叶斯网络.md.html">29 有向图模型：贝叶斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/30%20%e6%97%a0%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%9a%8f%e6%9c%ba%e5%9c%ba.md.html" id="30 无向图模型：马尔可夫随机场.md.html">30 无向图模型：马尔可夫随机场.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/31%20%e5%bb%ba%e6%a8%a1%e8%bf%9e%e7%bb%ad%e5%88%86%e5%b8%83%ef%bc%9a%e9%ab%98%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="31 建模连续分布：高斯网络.md.html">31 建模连续分布：高斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/32%20%e4%bb%8e%e6%9c%89%e9%99%90%e5%88%b0%e6%97%a0%e9%99%90%ef%bc%9a%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b.md.html" id="32 从有限到无限：高斯过程.md.html">32 从有限到无限：高斯过程.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/33%20%e5%ba%8f%e5%88%97%e5%8c%96%e5%bb%ba%e6%a8%a1%ef%bc%9a%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b.md.html" id="33 序列化建模：隐马尔可夫模型.md.html">33 序列化建模：隐马尔可夫模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/34%20%e8%bf%9e%e7%bb%ad%e5%ba%8f%e5%88%97%e5%8c%96%e6%a8%a1%e5%9e%8b%ef%bc%9a%e7%ba%bf%e6%80%a7%e5%8a%a8%e6%80%81%e7%b3%bb%e7%bb%9f.md.html" id="34 连续序列化模型：线性动态系统.md.html">34 连续序列化模型：线性动态系统.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/35%20%e7%b2%be%e7%a1%ae%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e9%87%8f%e6%b6%88%e9%99%a4%e5%8f%8a%e5%85%b6%e6%8b%93%e5%b1%95.md.html" id="35 精确推断：变量消除及其拓展.md.html">35 精确推断：变量消除及其拓展.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/36%20%e7%a1%ae%e5%ae%9a%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e5%88%86%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="36 确定近似推断：变分贝叶斯.md.html">36 确定近似推断：变分贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/37%20%e9%9a%8f%e6%9c%ba%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9aMCMC.md.html" id="37 随机近似推断：MCMC.md.html">37 随机近似推断：MCMC.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/38%20%e5%ae%8c%e5%a4%87%e6%95%b0%e6%8d%ae%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9a%e6%9c%89%e5%90%91%e5%9b%be%e4%b8%8e%e6%97%a0%e5%90%91%e5%9b%be.md.html" id="38 完备数据下的参数学习：有向图与无向图.md.html">38 完备数据下的参数学习：有向图与无向图.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/39%20%e9%9a%90%e5%8f%98%e9%87%8f%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9aEM%e6%96%b9%e6%b3%95%e4%b8%8e%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b.md.html" id="39 隐变量下的参数学习：EM方法与混合模型.md.html">39 隐变量下的参数学习：EM方法与混合模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/40%20%e7%bb%93%e6%9e%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e5%9f%ba%e4%ba%8e%e7%ba%a6%e6%9d%9f%e4%b8%8e%e5%9f%ba%e4%ba%8e%e8%af%84%e5%88%86.md.html" id="40 结构学习：基于约束与基于评分.md.html">40 结构学习：基于约束与基于评分.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e5%a6%82%e4%bd%95%e6%88%90%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%b7%a5%e7%a8%8b%e5%b8%88%ef%bc%9f.md.html" id="如何成为机器学习工程师？.md.html">如何成为机器学习工程师？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 机器学习的模型体系.md.html">总结课 机器学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 贝叶斯学习的模型体系.md.html">总结课 贝叶斯学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e7%bb%93%e8%af%be%20%e7%bb%88%e6%9c%89%e4%b8%80%e5%a4%a9%ef%bc%8c%e4%bd%a0%e5%b0%86%e4%b8%ba%e4%bb%8a%e5%a4%a9%e7%9a%84%e4%bb%98%e5%87%ba%e9%aa%84%e5%82%b2.md.html" id="结课 终有一天，你将为今天的付出骄傲.md.html">结课 终有一天，你将为今天的付出骄傲.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="19 非参数化的局部模型：K近邻" id="title">19 非参数化的局部模型：K近邻</h1>
<div><p>到目前为止，专栏中介绍的机器学习模型都属于参数模型，它们利用训练数据求解出关于问题的一般性知识，再将这些知识通过全局性模型的结构和参数加以外化。</p>
<p>一旦模型的结构和参数被确定，它们就不再依赖训练数据，可以直接用于未知数据的预测。而径向基核的出现一定程度上打破了这种规律，它将普适的全局特性打散成若干局部特性的组合，每个局部特性只能在它所覆盖的近邻区域内得以保持，由此产生的非结构化模型会具有更加灵活的表示能力。</p>
<p>在我看来，<strong>局部化的核心作用是模型复杂度和拟合精确性的折中</strong>。如果将整个输入空间看作一个大的整体区间，对它进行全局式的建模，那么单个模型就足以描述输入输出之间的规律，但这不可避免地会对表达能力造成较大的限制。</p>
<p>一个极端的情形是让所有输入的输出都等于同一个常数，这样的模型显然毫无信息量可言。可是在另一个极端，如果将局部特性继续加以细化，细化到让每个数据点都定义出不同局部特性的子区间，其结果就是基于实例的学习。</p>
<p><strong>基于实例的学习</strong>（instance-based learning）也叫<strong>基于记忆的学习</strong>（memory-based learning），<strong>它学习的不是明确的泛化模型，而是样本之间的关系</strong>。</p>
<p>当新的样本到来时，这种学习方式不会用拟合好的算式去计算输出结果或是输出结果的概率，而是根据这个新样本和训练样本之间的关系来确定它的输出。在本地化的语境里，这就叫“近朱者赤，近墨者黑”。</p>
<p>在基于实例的学习方法中，最典型的代表就是<span class="math inline">\(k\)</span>近邻。<span class="math inline">\(k\)</span>近邻算法（<span class="math inline">\(k\)</span>-nearest neighbors algorithm）可能是最简单的机器学习算法，它将每个训练实例都表示成高维特征空间中的一个点。</p>
<p>在解决分类问题时，<span class="math inline">\(k\)</span>近邻算法先找到高维空间中与未知实例最接近的<span class="math inline">\(k\)</span>个训练实例，再根据少数服从多数的原则，将这<span class="math inline">\(k\)</span>个实例中出现最多的类别标签分配给未知的实例。从贝叶斯定理的角度看，按照少数服从多数分配标签与后验概率最大化是等效的。</p>
<p>下图是<span class="math inline">\(k\)</span>近邻算法的一个简单的例子。训练数据属于两个不同的类别，分别用蓝色方框和红色三角表示，绿色圆圈则代表待分类的数据点，其类别由<span class="math inline">\(k\)</span>近邻算法决定。可以看到，当<span class="math inline">\(k\)</span>等于3时，离未知数据最近的三个点是两红一蓝，因此数据会被归类为红色三角。可是当<span class="math inline">\(k\)</span>从3增加到5时，多出来的两个实例都是蓝色的，这无疑会导致分类结果发生逆转。</p>
<p><img alt="" src="assets/e780b42d95a9d577c264fa1183b571ef.png"/></p>
<p>k近邻算法示意图（图片来自维基百科）</p>
<p>这个例子说明了<span class="math inline">\(k\)</span>近邻算法的一个特点，就是超参数<span class="math inline">\(k\)</span>对性能的影响。作为一种局部加权模型，<span class="math inline">\(k\)</span>近邻并不形成关于数据生成机制的全局性假设，而是刻画了数据在不同局部结构上的规律，局部结构的范围就是由<span class="math inline">\(k\)</span>来定义的。</p>
<p>在本质上，超参数<span class="math inline">\(k\)</span>和径向基核（以及其他的核函数）是一样的，只不过径向基核在定义局部结构时使用了连续分布的权值，所有数据对局部特征都有或大或小的贡献；而<span class="math inline">\(k\)</span>近邻使用了离散分布的权值，只有部分足够接近的数据才有资格定义局部特征，它可以视为是可变带宽的径向基核。</p>
<p>从另一个角度看，超参数<span class="math inline">\(k\)</span>表示了模型的复杂度，准确地说是和模型的复杂度成反比关系。如果训练集的容量为<span class="math inline">\(N\)</span>，算法的有效参数数目就可以近似表示为<span class="math inline">\(N / k\)</span>。</p>
<p><span class="math inline">\(k\)</span>均值的分类结果实质上是近邻区域内（就是上图中的圆圈）多个训练实例的平均，越大的<span class="math inline">\(k\)</span>值意味着近邻区域包含的点数越多，平均化的程度就越高，对训练实例中噪声的平滑效果也就越好，相应的模型复杂度就越低。<span class="math inline">\(k\)</span>的一个极端取值是直接等于训练集的容量，这相当于所有数据共同定义了同一个局部结构，这时的<span class="math inline">\(k\)</span>近邻就退化成稳定的全局模型了。</p>
<p>反过来，越小的<span class="math inline">\(k\)</span>值意味着近邻区域越狭窄，平均化的程度也就越低。这时的分类结果只由离未知数据点最近的少量训练实例决定，因而更容易受到这些实例中噪声的影响，也会表现出更强的过拟合倾向。当<span class="math inline">\(k\)</span>等于1时，未知数据的类别只取决于离它最近的训练实例。</p>
<p>这时画出每个训练实例的近邻边界，所有的近邻边界共同构成了对特征空间的<strong>Voronoi划分</strong>（Voronoi tessellation）。当训练实例较多时，这种1近邻算法计算出的分类边界会非常复杂，其泛化性能较差。</p>
<p>除了超参数<span class="math inline">\(k\)</span>之外，<span class="math inline">\(k\)</span>近邻算法的另一个变量是对距离的定义方式，也就是如何衡量哪些点才是“近邻”的标准。最常用的距离度量无疑是<strong>欧氏距离</strong>，可除此之外，<strong>闵可夫斯基距离</strong>（Minkowski distance）、<strong>曼哈顿距离</strong>（Manhattan distance）和<strong>马氏距离</strong>（Mahalanobis distance）也可以应用在<span class="math inline">\(k\)</span>近邻算法中，不同的距离代表的是对相似性的不同理解，在不同意义的相似性下，分类结果往往也会有所区别。这些距离是如何来定义的，你可以自己查阅。</p>
<p>对距离的依赖性给<span class="math inline">\(k\)</span>近邻算法带来了一个新问题，那就是<strong>维数灾难</strong>。在之前介绍维数灾难时我曾经留了一个扣，现在就该解开它了。简而言之，维数灾难对<span class="math inline">\(k\)</span>近邻算法的影响在于在高维空间中，曾经的近邻没有那么“近”了。不管特征空间的维度是多少，近邻区域的维度和特征空间的维度都是一致的。在这个前提下，要在特征维度增加时维持对特征空间的覆盖率不变，近邻区域在每个维度上的尺度就会越来越大。</p>
<p>维数灾难的几何意义其实可以直观地想象出来。如果特征空间是一条长度为1的一维直线，那任意一个长度为0.1的线段都能覆盖特征空间上10%的区域。可一旦特征空间变成二维，要在边长为1的正方形里圈出一个面积为0.1的小正方形，小正方形的边长就增加到<span class="math inline">\(\\sqrt{0.1} = 0.316\)</span>。这样一来，当数据点的数目不变时，维度的升高会导致原始的低维近邻点变得越来越稀疏，由近邻点所定义的局部结构也会越来越大。这样的局部结构失去了局部的意义，想让算法保证精确的分类性能就越困难了。</p>
<p><img alt="" src="assets/6c46d7f840c8e8badd37405d924fdc84.png"/></p>
<p>维度的升高会导致原始的低维近邻点变得越来越稀疏</p>
<p>（图片来自<a href="http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/）" target="_blank">http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/）</a></p>
<p>解决维数灾难最直接有效的方式就是<strong>增加数据点的数量</strong>，当数据点的数量没法增加时就得退而求其次，想办法<strong>降低特征空间的维度</strong>。还记得降维的主要方法吗？特征选择和特征提取都可以用于<span class="math inline">\(k\)</span>近邻算法对数据的预处理。</p>
<p>作为典型的非参数方法，<span class="math inline">\(k\)</span>近邻算法的运行方式和以线性回归为代表的参数方法截然相反。线性回归的运算量主要花在参数拟合上，利用大量的训练数据来拟合出使均方误差最小的一组参数。</p>
<p>一旦这组参数被计算出来，训练数据的历史使命就完成了，新来的数据都会用这组参数来处理。可<span class="math inline">\(k\)</span>近邻算法的训练过程没那么复杂，只需要将数据存储下来就行了。可是对新实例进行分类时，<span class="math inline">\(k\)</span>近邻算法需要找到离它最近的<span class="math inline">\(k\)</span>个训练实例，这才是算法主要的运算负荷。</p>
<p>虽然是频率主义的方法，但以核函数和<span class="math inline">\(k\)</span>近邻为代表的非参数方法也可以用来完成贝叶斯统计中<strong>概率密度估计</strong>（density estimation）的任务。如果用<strong>参数方法</strong>来进行密度估计，就需要先确定待估计概率密度的形式，再根据训练数据计算表示数字特征的参数。</p>
<p>比如假定概率密度具有正态分布的形式，那就需要估计它的均值和方差；具有指数分布的形式就要估计指数分布的参数。显然，参数化密度估计对概率密度形式的假设具有很强的依赖性，如果对概率分布的形式判断错误，那系数计算得再精确也是南辕北辙。</p>
<p>相比之下，非参数的密度估计就不会对待估计的分布做出什么先验假设，只是假定所有数据满足独立同分布的条件，因而具有更高的灵活性。但要讨论非参数密度估计方法，还是得从一种参数方法——<strong>直方图法</strong>（histogram）说起。</p>
<p>在统计学生成绩时，通常会计算&lt;60、60~70、70~80、80~90和&gt;90这些分数段内各有多少人，来大致绘出成绩的分布情况，这就是典型的直方图。直方图法将样本的取值范围划分成若干个等间隔子区间，再统计出现在每个子区间上的样本数目。在直方图上，第<span class="math inline">\(i\)</span>个子区间上的概率可以表示成</p>
<p><span class="math display">\[ p_i = \\dfrac{n_i}{N\\Delta} \]</span></p><p>其中<span class="math inline">\(n_i\)</span>是落在这个子区间内的样本数，<span class="math inline">\(N\)</span>是样本容量，<span class="math inline">\(\\Delta\)</span>是每个子区间的宽度，它决定了直方图的分辨率。<span class="math inline">\(\\Delta\)</span>的取值过小会让直方图过于细密，让过多的局部细节掩盖了分布的整体结构；取值过大又会让直方图过于平滑，体现不出潜在的多模式趋势。要对概率密度做出精确的估计，必须要选择合适的区间宽度。</p>
<p><strong>直方图方法的一个问题在于计算出的概率密度不是连续函数</strong>。要解决这个问题，可以将原始的子区间替换成平滑的连续函数，这就让整体概率密度等于所有局部概率密度的叠加，避免了不连续点的出现。</p>
<p>那么用来插值的连续函数应该满足什么样的条件呢？平滑特性决定了它不能只管自己，也要刻画数据的局部特性。那么刻画局部特性的工具是什么呢？核函数嘛！把核函数用于密度估计，就是非参数的<strong>核密度估计方法</strong>（kernel density estimation）。</p>
<p><img alt="" src="assets/9324a3f05f928041202d39dc4624cfa4.png"/></p>
<p>直方图（左）与核密度估计（右）（图片来自维基百科）</p>
<p>可有了核函数还不够，还需要确定它的带宽。在密度估计中，核函数带宽决定了局部结构的范围，其作用和直方图的子区间宽度类似。核函数的带宽过大会导致过度平滑，带宽过小则会导致欠平滑，在实际应用中可以通过最优化类似均方误差的指标来确定。</p>
<p>核密度估计虽然能够给出连续的概率密度，但它所有的局部结构都由相同的带宽决定。可是在特征空间上，不同区域数据的密度不同，其局部结构也应该有所区别，这时就需要引入<span class="math inline">\(k\)</span>近邻算法的思想。在基于近邻的密度估计中，近邻点的数目<span class="math inline">\(k\)</span>是唯一的参数，每个数据点的带宽就是第<span class="math inline">\(k\)</span>个最近点和它的距离。和核密度估计的带宽一样，<span class="math inline">\(k\)</span>值同样定义了局部结构的性质，因此在选择时也要慎之又慎。</p>
<p>核密度估计和近邻密度估计可以从一个统一的宏观视角加以审视。在高维空间中，如果将数据<span class="math inline">\(\\bf x\)</span>的局部结构定义为<span class="math inline">\(R\)</span>，那么其概率密度就可以表示为</p>
<p><span class="math display">\[ p({\\bf x}) = \\dfrac{K}{NV} \]</span></p><p>其中<span class="math inline">\(K\)</span>表示<span class="math inline">\(R\)</span>中的数据点数目，<span class="math inline">\(V\)</span>表示<span class="math inline">\(R\)</span>的体积，它们都是不确定的量。如果设置<span class="math inline">\(V\)</span>固定、<span class="math inline">\(K\)</span>可变来估计概率密度，得到的就是核密度估计；如果设置<span class="math inline">\(K\)</span>固定、<span class="math inline">\(V\)</span>可变来估计概率密度，得到的就是近邻密度估计。</p>
<p>当样本容量<span class="math inline">\(N \\rightarrow +\\infty\)</span>时，<span class="math inline">\(V\)</span>会随着<span class="math inline">\(N\)</span>的增加而缩小，以保证<span class="math inline">\(R\)</span>上的概率密度为常数；<span class="math inline">\(K\)</span>则会随着<span class="math inline">\(N\)</span>的增加而增加，以保证<span class="math inline">\(R\)</span>上的概率密度存在明显的峰值。这时，两种非参数的估计结果都会收敛到真实的概率密度。</p>
<p>和支持向量机一样，<span class="math inline">\(k\)</span>近邻算法也可以用来解决分类问题。利用Scikit-learn中的KNeighborsClassifier类，可以计算出曼城-西布朗数据集中的分类边界，其中<span class="math inline">\(k\)</span>的取值分别被设置为1，7和15。可以看到，<span class="math inline">\(k = 1\)</span>时所有训练数据都能正确分类，而<span class="math inline">\(k = 15\)</span>时误分类率超过了10%。</p>
<p>这说明当超参数<span class="math inline">\(k\)</span>的取值逐渐变大时，训练数据的误分类率在不断提升，但计算出的分类边界也变得越来越平滑。这是偏差-方差折中的典型体现。</p>
<p><img alt="" src="assets/198e43d514ea7ae95d20ef0a249cd717.png"/></p>
<p>今天我和你分享了基于实例的学习方法，以及它的典型代表<span class="math inline">\(k\)</span>近邻算法，其要点如下：</p>
<ul>
<li><p>基于实例的学习方法学的不是明确的泛化模型，而是样本之间的关系；</p></li>
<li><p><span class="math inline">\(k\)</span>近邻算法是非参数的局部化模型，具有无需训练的优点，但分类新实例的计算复杂度较高；</p></li>
<li><p><span class="math inline">\(k\)</span>近邻算法的性能取决于超参数<span class="math inline">\(k\)</span>的取值和距离的定义方式；</p></li>
<li><p>核方法和近邻算法都可以用于数据的概率密度估计。</p></li>
</ul>
<p><span class="math inline">\(k\)</span>近邻方法是一种消极学习（lazy learning）方法，它并不会从训练数据中直接获取泛化决策，而是将它延迟到新样本出现的时候。相比之下，前面介绍的其他方法都属于积极学习（active learning）方法，在新样本出现前就做好了泛化工作。</p>
<p>那么，你觉得消极方法和积极方法有什么原理和性能上的优缺点呢？</p>
<p>欢迎分享你的观点。</p>
<p><img alt="" src="assets/c70b1d8ad0befe6c23c2d8ffc9b2f9d6.jpg"/></p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#acc0c0c095989d9d9c9beccbc1cdc5c082cfc3c1" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9359dded29cfb964',t:'MTc0NTU0MjQ0My4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>