<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="13 线性降维：主成分的使用" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>13 线性降维：主成分的使用 </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%e6%89%93%e9%80%9a%e4%bf%ae%e7%82%bc%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%bb%bb%e7%9d%a3%e4%ba%8c%e8%84%89.md.html" id="00 开篇词 打通修炼机器学习的任督二脉.md.html">00 开篇词 打通修炼机器学习的任督二脉.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/01%20%e9%a2%91%e7%8e%87%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="01 频率视角下的机器学习.md.html">01 频率视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/02%20%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="02 贝叶斯视角下的机器学习.md.html">02 贝叶斯视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/03%20%e5%ad%a6%e4%bb%80%e4%b9%88%e4%b8%8e%e6%80%8e%e4%b9%88%e5%ad%a6.md.html" id="03 学什么与怎么学.md.html">03 学什么与怎么学.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/04%20%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba.md.html" id="04 计算学习理论.md.html">04 计算学习理论.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/05%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%88%86%e7%b1%bb%e6%96%b9%e5%bc%8f.md.html" id="05 模型的分类方式.md.html">05 模型的分类方式.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/06%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%be%e8%ae%a1%e5%87%86%e5%88%99.md.html" id="06 模型的设计准则.md.html">06 模型的设计准则.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/07%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%aa%8c%e8%af%81%e6%96%b9%e6%b3%95.md.html" id="07 模型的验证方法.md.html">07 模型的验证方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/08%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87.md.html" id="08 模型的评估指标.md.html">08 模型的评估指标.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/09%20%e5%ae%9e%e9%aa%8c%e8%ae%be%e8%ae%a1.md.html" id="09 实验设计.md.html">09 实验设计.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/10%20%e7%89%b9%e5%be%81%e9%a2%84%e5%a4%84%e7%90%86.md.html" id="10 特征预处理.md.html">10 特征预处理.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/11%20%e5%9f%ba%e7%a1%80%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%9a%e4%b8%80%e5%85%83%e4%b8%8e%e5%a4%9a%e5%85%83.md.html" id="11 基础线性回归：一元与多元.md.html">11 基础线性回归：一元与多元.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/12%20%e6%ad%a3%e5%88%99%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9a%e6%94%b6%e7%bc%a9%e6%96%b9%e6%b3%95%e4%b8%8e%e8%be%b9%e9%99%85%e5%8c%96.md.html" id="12 正则化处理：收缩方法与边际化.md.html">12 正则化处理：收缩方法与边际化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/13%20%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e4%b8%bb%e6%88%90%e5%88%86%e7%9a%84%e4%bd%bf%e7%94%a8.md.html" id="13 线性降维：主成分的使用.md.html">13 线性降维：主成分的使用.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/14%20%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0.md.html" id="14 非线性降维：流形学习.md.html">14 非线性降维：流形学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/15%20%e4%bb%8e%e5%9b%9e%e5%bd%92%e5%88%b0%e5%88%86%e7%b1%bb%ef%bc%9a%e8%81%94%e7%b3%bb%e5%87%bd%e6%95%b0%e4%b8%8e%e9%99%8d%e7%bb%b4.md.html" id="15 从回归到分类：联系函数与降维.md.html">15 从回归到分类：联系函数与降维.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/16%20%e5%bb%ba%e6%a8%a1%e9%9d%9e%e6%ad%a3%e6%80%81%e5%88%86%e5%b8%83%ef%bc%9a%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b.md.html" id="16 建模非正态分布：广义线性模型.md.html">16 建模非正态分布：广义线性模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/17%20%e5%87%a0%e4%bd%95%e8%a7%92%e5%ba%a6%e7%9c%8b%e5%88%86%e7%b1%bb%ef%bc%9a%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba.md.html" id="17 几何角度看分类：支持向量机.md.html">17 几何角度看分类：支持向量机.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/18%20%e4%bb%8e%e5%85%a8%e5%b1%80%e5%88%b0%e5%b1%80%e9%83%a8%ef%bc%9a%e6%a0%b8%e6%8a%80%e5%b7%a7.md.html" id="18 从全局到局部：核技巧.md.html">18 从全局到局部：核技巧.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/19%20%e9%9d%9e%e5%8f%82%e6%95%b0%e5%8c%96%e7%9a%84%e5%b1%80%e9%83%a8%e6%a8%a1%e5%9e%8b%ef%bc%9aK%e8%bf%91%e9%82%bb.md.html" id="19 非参数化的局部模型：K近邻.md.html">19 非参数化的局部模型：K近邻.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/20%20%e5%9f%ba%e4%ba%8e%e8%b7%9d%e7%a6%bb%e7%9a%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e8%81%9a%e7%b1%bb%e4%b8%8e%e5%ba%a6%e9%87%8f%e5%ad%a6%e4%b9%a0.md.html" id="20 基于距离的学习：聚类与度量学习.md.html">20 基于距离的学习：聚类与度量学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/21%20%e5%9f%ba%e5%87%bd%e6%95%b0%e6%89%a9%e5%b1%95%ef%bc%9a%e5%b1%9e%e6%80%a7%e7%9a%84%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%8c%96.md.html" id="21 基函数扩展：属性的非线性化.md.html">21 基函数扩展：属性的非线性化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/22%20%e8%87%aa%e9%80%82%e5%ba%94%e7%9a%84%e5%9f%ba%e5%87%bd%e6%95%b0%ef%bc%9a%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c.md.html" id="22 自适应的基函数：神经网络.md.html">22 自适应的基函数：神经网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/23%20%e5%b1%82%e6%ac%a1%e5%8c%96%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%ef%bc%9a%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0.md.html" id="23 层次化的神经网络：深度学习.md.html">23 层次化的神经网络：深度学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/24%20%e6%b7%b1%e5%ba%a6%e7%bc%96%e8%a7%a3%e7%a0%81%ef%bc%9a%e8%a1%a8%e7%a4%ba%e5%ad%a6%e4%b9%a0.md.html" id="24 深度编解码：表示学习.md.html">24 深度编解码：表示学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/25%20%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e5%8c%ba%e5%9f%9f%e5%88%92%e5%88%86%ef%bc%9a%e6%a0%91%e6%a8%a1%e5%9e%8b.md.html" id="25 基于特征的区域划分：树模型.md.html">25 基于特征的区域划分：树模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/26%20%e9%9b%86%e6%88%90%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9aBoosting%e4%b8%8eBagging.md.html" id="26 集成化处理：Boosting与Bagging.md.html">26 集成化处理：Boosting与Bagging.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/27%20%e4%b8%87%e8%83%bd%e6%a8%a1%e5%9e%8b%ef%bc%9a%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97.md.html" id="27 万能模型：梯度提升与随机森林.md.html">27 万能模型：梯度提升与随机森林.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/28%20%e6%9c%80%e7%ae%80%e5%8d%95%e7%9a%84%e6%a6%82%e7%8e%87%e5%9b%be%ef%bc%9a%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="28 最简单的概率图：朴素贝叶斯.md.html">28 最简单的概率图：朴素贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/29%20%e6%9c%89%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="29 有向图模型：贝叶斯网络.md.html">29 有向图模型：贝叶斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/30%20%e6%97%a0%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%9a%8f%e6%9c%ba%e5%9c%ba.md.html" id="30 无向图模型：马尔可夫随机场.md.html">30 无向图模型：马尔可夫随机场.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/31%20%e5%bb%ba%e6%a8%a1%e8%bf%9e%e7%bb%ad%e5%88%86%e5%b8%83%ef%bc%9a%e9%ab%98%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="31 建模连续分布：高斯网络.md.html">31 建模连续分布：高斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/32%20%e4%bb%8e%e6%9c%89%e9%99%90%e5%88%b0%e6%97%a0%e9%99%90%ef%bc%9a%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b.md.html" id="32 从有限到无限：高斯过程.md.html">32 从有限到无限：高斯过程.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/33%20%e5%ba%8f%e5%88%97%e5%8c%96%e5%bb%ba%e6%a8%a1%ef%bc%9a%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b.md.html" id="33 序列化建模：隐马尔可夫模型.md.html">33 序列化建模：隐马尔可夫模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/34%20%e8%bf%9e%e7%bb%ad%e5%ba%8f%e5%88%97%e5%8c%96%e6%a8%a1%e5%9e%8b%ef%bc%9a%e7%ba%bf%e6%80%a7%e5%8a%a8%e6%80%81%e7%b3%bb%e7%bb%9f.md.html" id="34 连续序列化模型：线性动态系统.md.html">34 连续序列化模型：线性动态系统.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/35%20%e7%b2%be%e7%a1%ae%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e9%87%8f%e6%b6%88%e9%99%a4%e5%8f%8a%e5%85%b6%e6%8b%93%e5%b1%95.md.html" id="35 精确推断：变量消除及其拓展.md.html">35 精确推断：变量消除及其拓展.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/36%20%e7%a1%ae%e5%ae%9a%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e5%88%86%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="36 确定近似推断：变分贝叶斯.md.html">36 确定近似推断：变分贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/37%20%e9%9a%8f%e6%9c%ba%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9aMCMC.md.html" id="37 随机近似推断：MCMC.md.html">37 随机近似推断：MCMC.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/38%20%e5%ae%8c%e5%a4%87%e6%95%b0%e6%8d%ae%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9a%e6%9c%89%e5%90%91%e5%9b%be%e4%b8%8e%e6%97%a0%e5%90%91%e5%9b%be.md.html" id="38 完备数据下的参数学习：有向图与无向图.md.html">38 完备数据下的参数学习：有向图与无向图.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/39%20%e9%9a%90%e5%8f%98%e9%87%8f%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9aEM%e6%96%b9%e6%b3%95%e4%b8%8e%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b.md.html" id="39 隐变量下的参数学习：EM方法与混合模型.md.html">39 隐变量下的参数学习：EM方法与混合模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/40%20%e7%bb%93%e6%9e%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e5%9f%ba%e4%ba%8e%e7%ba%a6%e6%9d%9f%e4%b8%8e%e5%9f%ba%e4%ba%8e%e8%af%84%e5%88%86.md.html" id="40 结构学习：基于约束与基于评分.md.html">40 结构学习：基于约束与基于评分.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e5%a6%82%e4%bd%95%e6%88%90%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%b7%a5%e7%a8%8b%e5%b8%88%ef%bc%9f.md.html" id="如何成为机器学习工程师？.md.html">如何成为机器学习工程师？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 机器学习的模型体系.md.html">总结课 机器学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 贝叶斯学习的模型体系.md.html">总结课 贝叶斯学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e7%bb%93%e8%af%be%20%e7%bb%88%e6%9c%89%e4%b8%80%e5%a4%a9%ef%bc%8c%e4%bd%a0%e5%b0%86%e4%b8%ba%e4%bb%8a%e5%a4%a9%e7%9a%84%e4%bb%98%e5%87%ba%e9%aa%84%e5%82%b2.md.html" id="结课 终有一天，你将为今天的付出骄傲.md.html">结课 终有一天，你将为今天的付出骄傲.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="13 线性降维：主成分的使用" id="title">13 线性降维：主成分的使用</h1>
<div><p>在前一篇文章中，我以岭回归和LASSO为例介绍了线性回归的正则化处理。这两种方法都属于<strong>收缩方法</strong>（shrinkage method），它们能够使线性回归的系数连续变化。但和岭回归不同的是，LASSO可以将一部分属性的系数收缩为0，事实上起到了筛选属性的作用。</p>
<p>和LASSO这种间接去除属性的收缩方法相对应的是<strong>维度规约</strong>。维度规约这个听起来个高大上的名称是数据挖掘中常用的术语，它有一个更接地气的同义词，就是<strong>降维</strong>（dimensionality reduction），也就是直接降低输入属性的数目来削减数据的维度。</p>
<p>对数据维度的探讨来源于“<strong>维数灾难</strong>”（curse of dimensionality），这个概念是数学家理查德·贝尔曼（Richard Bellman）在动态优化问题的研究中提出的。</p>
<p>发表于《IEEE模式分析与机器智能汇刊》（IEEE Transactions on Pattern Analysis and Machine Intelligence）第1卷第3期的论文《维数问题：一个简单实例（A Problem of Dimensionality: A Simple Example）》在数学上证明了当所有参数都已知时，属性维数的增加可以让分类问题的错误率渐进为0；可当未知的参数只能根据数量有限的样本来估计时，属性维数的增加会使错误率先降低再升高，最终收敛到0.5。</p>
<p>这就像一群谋士七嘴八舌在支招，当领导的要是对每个人的意见都深入考虑再来拍板的话，这样的决策也没什么准确性可言了。</p>
<p><strong>维数灾难深层次的原因在于数据样本的有限</strong>。当属性的维数增加时，每个属性每个可能取值的组合就会以指数形式增长。对于二值属性来说，2个属性所有可能的取值组合共有4种，可每增加一个属性，可能的组合数目就会翻番。</p>
<p>一般的经验法则是每个属性维度需要对应至少5个数据样本，可当属性维数增加而样本数目不变时，过少的数据就不足以体现出属性背后的趋势，从而导致过拟合的发生。当然，这只是维数灾难的一种解释方式，另一种解释方式来源于几何角度的数据稀疏性，这里暂且按下不表。</p>
<p>在数据有限的前提下解决维数灾难问题，化繁为简的降维是必经之路。降维的对象通常是那些“食之无味，弃之可惜”的鸡肋属性。食之无味是因为它们或者和结果的相关性不强，或者和其他属性之间有较强的关联，使用这样的属性没有多大必要；弃之可惜则是因为它们终究还包含一些独有的信息，就这么断舍离又会心有不甘。</p>
<p>如果像亚历山大剑斩戈尔迪之结一般直接砍掉鸡肋属性，这种“简单粗暴”的做法就是<strong>特征选择</strong>（feature selection）。另一种更加稳妥的办法是破旧立新，将所有原始属性的信息一点儿不浪费地整合到脱胎换骨的新属性中，这就是<strong>特征提取</strong>（feature extraction）的方法。</p>
<p>无论是特征选择还是特征提取，在“人工智能基础课”中都有相应的介绍。</p>
<p>今天我要换个角度，先从刚刚介绍过的岭回归说起。假设数据集中有<span class="math inline">\(N\)</span>个样本，每个样本都有<span class="math inline">\(p\)</span>个属性，则数据矩阵<span class="math inline">\(\\bf X\)</span>的维度就是<span class="math inline">\(N \\times p\)</span>。将中心化处理后，也就是减去每个属性平均值的<span class="math inline">\(\\bf X\)</span>进行奇异值分解（singular value decomposition）可以得到</p>
<p><span class="math display">\[ {\\bf X} = {\\bf U} {\\bf D} {\\bf V}^T \]</span></p><p>其中的<span class="math inline">\(\\bf U\)</span>和<span class="math inline">\(\\bf V\)</span>分别是<span class="math inline">\(N \\times p\)</span>维和<span class="math inline">\(p \\times p\)</span>维的正交矩阵，其各自的所有列向量可以张成一个子空间；<span class="math inline">\(\\bf D\)</span>则是对角矩阵，对角线上的各个元素是数据矩阵<span class="math inline">\(\\bf X\)</span>按从大到小顺序排列的奇异值（singular value）<span class="math inline">\(d_j\)</span>。可以证明，岭回归求出的最优系数可以写成<span class="math inline">\(\\hat \\beta = ({\\bf X}^T {\\bf X} + \\lambda {\\bf I}) ^ {-1} {\\bf X}^T {\\bf y}\)</span>。将<span class="math inline">\(\\bf X\)</span>的奇异值分解代入岭回归的预测输出中，就可以得到：</p>
<p><span class="math display">\[ {\\bf X} {\\hat \\beta} = \\sum\\limits_{j = 1}^p {\\bf u}_j \\dfrac{d^2_j}{d^2_j + \\lambda}{\\bf u}^T_j {\\bf y}\]</span></p><p>其中的<span class="math inline">\({\\bf u}_j\)</span>是矩阵<span class="math inline">\(\\bf U\)</span>的列向量，也是<span class="math inline">\(\\bf X\)</span>的列空间的一组正交基，而岭回归计算出的结果正是将训练数据的输出<span class="math inline">\(\\bf y\)</span>投影到以<span class="math inline">\({\\bf u}_j\)</span>为正交基的子空间上所得到的坐标。除了空间变换之外，岭回归的收缩特性也有体现，那就是上式中的系数。当正则化参数<span class="math inline">\(\\lambda\)</span>一定时，奇异值<span class="math inline">\(d_j\)</span>越小，它对应的坐标被衰减地就越厉害。</p>
<p>除了经历不同的收缩外，奇异值<span class="math inline">\(d_j\)</span>还有什么意义呢？<span class="math inline">\(d_j\)</span>的平方可以写成对角矩阵<span class="math inline">\(\\bf D\)</span>的平方，利用奇异值分解又可以推导出<span class="math inline">\({\\bf D}^2\)</span>和数据矩阵<span class="math inline">\(\\bf X\)</span>如下的关系</p>
<p><span class="math display">\[ {\\bf X}^T{\\bf X} = {\\bf V} {\\bf D}^2 {\\bf V}^T \]</span></p><p>这个表达式实际上就是矩阵的<strong>特征分解</strong>（eigen decomposition）：等式左侧的表达式实际上就是数据的协方差矩阵（covariance matrix）乘以维度<span class="math inline">\(N\)</span>，<span class="math inline">\(\\bf V\)</span>中的每一列<span class="math inline">\(v_j\)</span>都是协方差矩阵的特征向量，<span class="math inline">\({\\bf D}^2\)</span>中的每个对角元素<span class="math inline">\(d^2_j\)</span>则是对应的特征值。如果你对主成分分析还有印象，就不难发现每一个<span class="math inline">\({\\bf X}{\\bf v}_j\)</span>都是一个主成分（principal component），第<span class="math inline">\(j\)</span>个主成分上数据的方差就是<span class="math inline">\(d^2_j / N\)</span>。</p>
<p>解释到这儿，就能够看出岭回归的作用了：<strong>岭回归收缩系数的对象并非每个单独的属性，而是由属性的线性组合计算出来的互不相关的主成分，主成分上数据的方差越小，其系数收缩地就越明显</strong>。</p>
<p>数据在一个主成分上波动较大意味着主成分的取值对数据有较高的区分度，也就是上一季中提到的“最大方差原理”。反之，数据在另一个主成分上方差较小就说明不同数据的取值较为集中，而聚成一团的数据显然是不容易区分的。岭回归正是通过削弱方差较小的主成分、保留方差较大的主成分来简化模型，实现正则化的。</p>
<p><img alt="" src="assets/ea5287f918ab983e30607dc988839264.png">-
﻿﻿不同方差的主成分示意图，2点钟方向的主成分方差较大，11点钟方向的主成分方差较小（图片来自维基百科）</img></p>
<p>看到这里你可能就想到了：既然主成分都已经算出来了，与其用岭回归兜一个圈子，莫不如直接使用它们作为自变量来计算线性回归的系数，这种思路得到的就是<strong>主成分回归</strong>（principal component regression）。</p>
<p>主成分回归以每个主成分<span class="math inline">\({\\bf z}_j = {\\bf X} {\\bf v}_j\)</span>作为输入计算回归参数。由于不同的主成分是两两正交的，因此这个看似多元线性回归的问题实质上是多个独立的简单线性回归的组合，每个主成分的权重系数可以表示为</p>
<p><span class="math display">\[ \\hat \\theta_m = \\dfrac{&lt;{\\bf z}_m, {\\bf y}&gt;}{&lt;{\\bf z}_m, {\\bf z}_m&gt;} \]</span></p><p>其中<span class="math inline">\(&lt;&gt;\)</span>表示内积运算。需要注意的是这里的<span class="math inline">\(\\bf y\)</span>和数据矩阵的每一列<span class="math inline">\({\\bf x}_j\)</span>都要做去均值的处理，主成分回归的常数项就是<span class="math inline">\(\\bf y\)</span>，也就是所有数据输出结果的均值<span class="math inline">\(\\bar y\)</span>。</p>
<p>当主成分回归中使用的主成分数目等于数据的属性数目<span class="math inline">\(p\)</span>时，主成分回归和岭回归的结果是一致的。可如果放弃方差最小的若干个主成分，得到的就是约化的回归结果，从而更加清晰地体现出主成分分析的思想。</p>
<p>主成分分析是典型的特征提取方法，它和收缩方法的本质区别在于将原始的共线性特征转化为人为生成的正交特征，从而带来了数据维度的约简和数据压缩的可能性。数字图像处理中的特征脸方法是主成分回归最典型的应用之一。</p>
<p>所谓“<strong>特征脸</strong>”（eigenface）实际上就是用于人脸识别的主成分。用特征脸方法处理的人脸图像都具有相同的空间维度，假定图像的像素数目都是<span class="math inline">\(100 \\times 100\)</span>，那么每一个像素点都是一个属性，数字图像就变成了10000维空间中的一个点。可一般数字图像慢变特性决定了这10000个特征之间具有很强的关联，直接处理的话运算量较大不说，还未必有好的效果，可谓事倍功半。</p>
<p><img alt="" src="assets/e6fd81d7312849205e57e1007c792037.png"/>-
﻿﻿根据AT&amp;T Laboratories Cambridge Facedatabase人脸数据库生成的特征脸</p>
<p>图片来自<a href="https://www.bytefish.de/blog/eigenfaces/" target="_blank">https://www.bytefish.de/blog/eigenfaces/</a></p>
<p>引入主成分分析后，情况就不一样了。主成分分析可以将10000个相互关联的像素维度精炼成100～150个特征脸，再用这些特征脸来重构相同形状的人脸图像。</p>
<p>上图是真实计算出的一组特征脸图像，如果是晚上一个人在家玩手机的话，那这组惊悚的特征脸很可能让你吓得不轻。可如果你能想明白一个问题：这只是一组被打成正方形的10000多维的相互正交的主成分，而原始图像让它们碰巧具有了人脸的轮廓，这些人不人鬼不鬼的东西就没有那么恐怖了。</p>
<p>这些主成分可以用来分解任意一张面孔，说不定我的一寸照片就可以表示成<span class="math inline">\(27% \\times Ef_1 + 6% \\times Ef_2 + 49% \\times Ef_3 + \\cdots + 34% \\times Ef_{16}\)</span>的组合呢。</p>
<p>前面对主成分分析的解释都是从降维的角度出发的。换个角度，主成分分析可以看成<strong>对高斯隐变量的概率描述</strong>。隐变量（latent variable）是不能直接观测但可以间接推断的变量，虽然数据本身处在高维空间之中，但决定其变化特点的可能只是有限个参数，这些幕后的操纵者就是隐变量，寻找隐变量的过程就是对数据进行降维的过程。</p>
<p><strong>概率主成分分析</strong>（probabilistic principal component analysis）体现的就是高斯型观测结果和高斯隐变量之间线性的相关关系，它是因子分析（factor analysis）的一个特例。概率主成分分析的数学推导比较复杂，在这里不妨直接给出结论：</p>
<p>假定一组数据观测值构成了<span class="math inline">\(D\)</span>维向量<span class="math inline">\(\\bf X\)</span>，另外一组隐变量构成了<span class="math inline">\(Q\)</span>维向量<span class="math inline">\(\\bf Z\)</span>，两者之间的线性关系就可以表示为</p>
<p><span class="math display">\[ {\\bf X} = {\\bf W} {\\bf Z} + \\boldsymbol \\mu + \\boldsymbol \\epsilon\]</span></p><p>其中关联矩阵<span class="math inline">\(\\bf W\)</span>是由标准正交基构成的矩阵，非零向量<span class="math inline">\(\\boldsymbol \\mu\)</span>表示观测值的均值，<span class="math inline">\(\\boldsymbol \\epsilon\)</span>则是服从标准多维正态分布<span class="math inline">\(N({\\bf 0}, \\sigma^2{\\bf I})\)</span>的各向同性的噪声。如果假设隐变量<span class="math inline">\(\\bf Z\)</span>具有多元标准正态形式的先验分布<span class="math inline">\(p({\\bf Z})\)</span>，去均值观测数据<span class="math inline">\({\\bf X}\)</span>的对数似然概率可以写成</p>
<p><span class="math display">\[ \\log p({\\bf X} | {\\bf W}, \\sigma^2) = - \\dfrac{N}{2} \\ln |{\\bf C}| - \\dfrac{1}{2} \\sum\\limits_{i = 1}^N {\\bf x}_i^T {\\bf C}^{-1} {\\bf x}_i\]</span></p><p>其中<span class="math inline">\({\\bf C} = {\\bf W}{\\bf W}^T + \\sigma^2{\\bf I}\)</span>。计算可得，让似然概率取得最大值的参数值为<span class="math inline">\(\\hat {\\bf W} = {\\bf V}({\\bf D}^ 2 - \\sigma^2{\\bf I}) ^ {1/2}\)</span>。根据这个<span class="math inline">\(\\hat {\\bf W}\)</span>又可以计算出超参数<span class="math inline">\(\\sigma ^ 2\)</span>得最大似然估计值<span class="math inline">\(\\hat \\sigma ^ 2 = \\dfrac{1}{D - Q} \\sum\\limits_{j = Q + 1}^D d_j^2\)</span>，这说明噪声方差就是所有被丢弃的主成分方差的均值。而当<span class="math inline">\(\\boldsymbol \\epsilon \\rightarrow 0\)</span>时，概率主成分分析的线性系数<span class="math inline">\(\\hat {\\bf W}\)</span>就会退化为经典的主成分分析中的<span class="math inline">\(\\bf V\)</span>。</p>
<p>除了似然概率外，根据正态分布的性质也可以计算出隐变量的后验概率<span class="math inline">\(p({\\bf Z} | {\\bf X})\)</span>。令<span class="math inline">\(\\hat {\\bf F} = \\hat {\\bf W}^T \\hat {\\bf W} + \\hat \\sigma^2 {\\bf I}\)</span>，后验概率满足的就是以<span class="math inline">\(\\hat {\\bf F} ^ {-1}\\hat {\\bf W} {\\bf X}\)</span>为均值，<span class="math inline">\(\\sigma ^ 2 \\hat {\\bf F}^{-1}\)</span>为方差的正态分布。当<span class="math inline">\(\\boldsymbol \\epsilon \\rightarrow 0\)</span>时，隐变量的最优值就会收敛为经典主成分<span class="math inline">\({\\bf XV}\)</span>。</p>
<p>在实际问题中，使用的主成分数目是个超参数，需要通过模型选择确定，而概率主成分分析对测试数据的处理就可以完成模型选择的任务。从重构误差的角度出发，经典主成分分析一般会让被选中的主成分的特征值之和占所有特征值之和的95%以上。在贝叶斯框架下，计算最优的主成分数目需要对所有未知的参数超参数进行积分，其过程非常复杂，在这里就不讨论了。</p>
<p>同其他隐变量模型一样，概率主成分分析也是个生成模型，其生成机制如下图所示。首先从服从一维正态分布的隐变量<span class="math inline">\(z\)</span>中得到采样值<span class="math inline">\(\\hat z\)</span>，以<span class="math inline">\({\\bf w}z + \\boldsymbol \\mu\)</span>为均值的单位方差二维正态分布就是数据<span class="math inline">\(\\bf x\)</span>的似然分布，将先验分布与似然分布相乘，得到的就是最右侧的二维分布<span class="math inline">\(p(\\bf x)\)</span>了。</p>
<p><img alt="" src="assets/92ea88cc40155f4fa2e50b4e24c891e9.png"/></p>
<p>﻿﻿概率主成分分析表示的数据生成机制</p>
<p>图片来自Machine Learning: A Probabilistic Perspective, 图12.1</p>
<p>在Scikit-learn中，主成分分析对应的类是PCA，它在decomposition的模块种。还是以英超数据集为例，对多元线性回归的数据进行主成分分析，可以得到10个主成分的方差，以及它们占总方差的比例：</p>
<p><img alt="" src="assets/390f6ca36bd154f9d0d61e58f79b8891.jpeg"/></p>
<p>英超数据集上所有主成分的方差及其比例</p>
<p>从结果中可以看出，方差最大的主成分占据了近4/5的总方差，前两个主成分的方差之和的比例则超过了90%。在对数据进行降维时，如果将方差的比例阈值设定为95%，保留的主成分数目就是2个，这说明2个主成分已经足以解释输出结果中90%的变化。</p>
<p><img alt="" src="assets/31f4e937adea5a8ed8ce4edb19a95d48.png"/></p>
<p>使用前两个主成分对英超数据集进行变换的结果</p>
<p>为了对主成分分析后的数据分布产生直观的认识，可以将变换后的数据点显示在低维空间中，以观察它们的集中程度。出于观察方便的考虑，在可视化时只选择了方差最大的前两个主成分，虽然这样做会造成较大的误差，但变换后的数据就可以在平面直角坐标系上显示出来，如上图所示。</p>
<p>可以看出，经过变换后的数据点依然分散在整个二维平面上，但根据它们在横轴上的取值已经可以近似地将数据划分为两个类别，其原因很可能是蓝线两侧的数据代表了两种类型的球队风格，就像来自两个高斯分布的随机数。</p>
<p>今天我和你分享了从岭回归到主成分回归的推导过程，以及作为降维方法和特征提取技术的主成分分析，其要点如下：</p>
<ul>
<li><p>在有限的数据集下，数据维度过高会导致维数灾难；</p></li>
<li><p>降维的方法包括特征选择和特征提取；</p></li>
<li><p>主成分分析将原始的共线性特征转化为新的正交特征，从而实现特征提取；</p></li>
<li><p>概率主成分分析是因子分析的一种，是数据的生成模型。</p></li>
</ul>
<p>在机器学习中，还有一种和主成分分析名字相似的方法，叫作<strong>独立成分分析</strong>（independent component analysis）。那么这两者之间到底有什么区别和联系呢？</p>
<p>你可以查阅资料加以了解，并在这里分享你的理解。</p>
<p><img alt="" src="assets/58c45c5e94e8d8c8f05e4f355790cbd9.jpg"/></p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#f8949494c1ccc9c9c8cfb89f95999194d69b9795" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9359dd5a2d5005c1',t:'MTc0NTU0MjQxOS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>