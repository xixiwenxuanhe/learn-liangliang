<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" name="viewport"/>
<meta content="zh-cn" http-equiv="content-language"/>
<meta content="37 随机近似推断：MCMC" name="description"/>
<link href="/static/favicon.png" rel="icon"/>
<title>37 随机近似推断：MCMC </title>
<link href="/static/index.css" rel="stylesheet"/>
<link href="/static/highlight.min.css" rel="stylesheet"/>
<script src="/static/highlight.min.js"></script>
<meta content="Hexo 4.2.0" name="generator"/>
<script data-website-id="83e5d5db-9d06-40e3-b780-cbae722fdf8c" defer="" src="https://umami.lianglianglee.com/script.js"></script>
</head>
<body>
<div class="book-container">
<div class="book-sidebar">
<div class="book-brand">
<a href="/">
<img src="/static/favicon.png"/>
<span>技术文章摘抄</span>
</a>
</div>
<div class="book-menu uncollapsible">
<ul class="uncollapsible">
<li><a class="current-tab" href="/">首页</a></li>
<li><a href="../">上一级</a></li>
</ul>
<ul class="uncollapsible">
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/00%20%e5%bc%80%e7%af%87%e8%af%8d%20%e6%89%93%e9%80%9a%e4%bf%ae%e7%82%bc%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%bb%bb%e7%9d%a3%e4%ba%8c%e8%84%89.md.html" id="00 开篇词 打通修炼机器学习的任督二脉.md.html">00 开篇词 打通修炼机器学习的任督二脉.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/01%20%e9%a2%91%e7%8e%87%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="01 频率视角下的机器学习.md.html">01 频率视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/02%20%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e4%b8%8b%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0.md.html" id="02 贝叶斯视角下的机器学习.md.html">02 贝叶斯视角下的机器学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/03%20%e5%ad%a6%e4%bb%80%e4%b9%88%e4%b8%8e%e6%80%8e%e4%b9%88%e5%ad%a6.md.html" id="03 学什么与怎么学.md.html">03 学什么与怎么学.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/04%20%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba.md.html" id="04 计算学习理论.md.html">04 计算学习理论.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/05%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%88%86%e7%b1%bb%e6%96%b9%e5%bc%8f.md.html" id="05 模型的分类方式.md.html">05 模型的分类方式.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/06%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%be%e8%ae%a1%e5%87%86%e5%88%99.md.html" id="06 模型的设计准则.md.html">06 模型的设计准则.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/07%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%aa%8c%e8%af%81%e6%96%b9%e6%b3%95.md.html" id="07 模型的验证方法.md.html">07 模型的验证方法.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/08%20%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87.md.html" id="08 模型的评估指标.md.html">08 模型的评估指标.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/09%20%e5%ae%9e%e9%aa%8c%e8%ae%be%e8%ae%a1.md.html" id="09 实验设计.md.html">09 实验设计.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/10%20%e7%89%b9%e5%be%81%e9%a2%84%e5%a4%84%e7%90%86.md.html" id="10 特征预处理.md.html">10 特征预处理.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/11%20%e5%9f%ba%e7%a1%80%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%ef%bc%9a%e4%b8%80%e5%85%83%e4%b8%8e%e5%a4%9a%e5%85%83.md.html" id="11 基础线性回归：一元与多元.md.html">11 基础线性回归：一元与多元.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/12%20%e6%ad%a3%e5%88%99%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9a%e6%94%b6%e7%bc%a9%e6%96%b9%e6%b3%95%e4%b8%8e%e8%be%b9%e9%99%85%e5%8c%96.md.html" id="12 正则化处理：收缩方法与边际化.md.html">12 正则化处理：收缩方法与边际化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/13%20%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e4%b8%bb%e6%88%90%e5%88%86%e7%9a%84%e4%bd%bf%e7%94%a8.md.html" id="13 线性降维：主成分的使用.md.html">13 线性降维：主成分的使用.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/14%20%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4%ef%bc%9a%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0.md.html" id="14 非线性降维：流形学习.md.html">14 非线性降维：流形学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/15%20%e4%bb%8e%e5%9b%9e%e5%bd%92%e5%88%b0%e5%88%86%e7%b1%bb%ef%bc%9a%e8%81%94%e7%b3%bb%e5%87%bd%e6%95%b0%e4%b8%8e%e9%99%8d%e7%bb%b4.md.html" id="15 从回归到分类：联系函数与降维.md.html">15 从回归到分类：联系函数与降维.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/16%20%e5%bb%ba%e6%a8%a1%e9%9d%9e%e6%ad%a3%e6%80%81%e5%88%86%e5%b8%83%ef%bc%9a%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b.md.html" id="16 建模非正态分布：广义线性模型.md.html">16 建模非正态分布：广义线性模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/17%20%e5%87%a0%e4%bd%95%e8%a7%92%e5%ba%a6%e7%9c%8b%e5%88%86%e7%b1%bb%ef%bc%9a%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba.md.html" id="17 几何角度看分类：支持向量机.md.html">17 几何角度看分类：支持向量机.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/18%20%e4%bb%8e%e5%85%a8%e5%b1%80%e5%88%b0%e5%b1%80%e9%83%a8%ef%bc%9a%e6%a0%b8%e6%8a%80%e5%b7%a7.md.html" id="18 从全局到局部：核技巧.md.html">18 从全局到局部：核技巧.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/19%20%e9%9d%9e%e5%8f%82%e6%95%b0%e5%8c%96%e7%9a%84%e5%b1%80%e9%83%a8%e6%a8%a1%e5%9e%8b%ef%bc%9aK%e8%bf%91%e9%82%bb.md.html" id="19 非参数化的局部模型：K近邻.md.html">19 非参数化的局部模型：K近邻.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/20%20%e5%9f%ba%e4%ba%8e%e8%b7%9d%e7%a6%bb%e7%9a%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e8%81%9a%e7%b1%bb%e4%b8%8e%e5%ba%a6%e9%87%8f%e5%ad%a6%e4%b9%a0.md.html" id="20 基于距离的学习：聚类与度量学习.md.html">20 基于距离的学习：聚类与度量学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/21%20%e5%9f%ba%e5%87%bd%e6%95%b0%e6%89%a9%e5%b1%95%ef%bc%9a%e5%b1%9e%e6%80%a7%e7%9a%84%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%8c%96.md.html" id="21 基函数扩展：属性的非线性化.md.html">21 基函数扩展：属性的非线性化.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/22%20%e8%87%aa%e9%80%82%e5%ba%94%e7%9a%84%e5%9f%ba%e5%87%bd%e6%95%b0%ef%bc%9a%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c.md.html" id="22 自适应的基函数：神经网络.md.html">22 自适应的基函数：神经网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/23%20%e5%b1%82%e6%ac%a1%e5%8c%96%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%ef%bc%9a%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0.md.html" id="23 层次化的神经网络：深度学习.md.html">23 层次化的神经网络：深度学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/24%20%e6%b7%b1%e5%ba%a6%e7%bc%96%e8%a7%a3%e7%a0%81%ef%bc%9a%e8%a1%a8%e7%a4%ba%e5%ad%a6%e4%b9%a0.md.html" id="24 深度编解码：表示学习.md.html">24 深度编解码：表示学习.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/25%20%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e5%8c%ba%e5%9f%9f%e5%88%92%e5%88%86%ef%bc%9a%e6%a0%91%e6%a8%a1%e5%9e%8b.md.html" id="25 基于特征的区域划分：树模型.md.html">25 基于特征的区域划分：树模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/26%20%e9%9b%86%e6%88%90%e5%8c%96%e5%a4%84%e7%90%86%ef%bc%9aBoosting%e4%b8%8eBagging.md.html" id="26 集成化处理：Boosting与Bagging.md.html">26 集成化处理：Boosting与Bagging.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/27%20%e4%b8%87%e8%83%bd%e6%a8%a1%e5%9e%8b%ef%bc%9a%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97.md.html" id="27 万能模型：梯度提升与随机森林.md.html">27 万能模型：梯度提升与随机森林.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/28%20%e6%9c%80%e7%ae%80%e5%8d%95%e7%9a%84%e6%a6%82%e7%8e%87%e5%9b%be%ef%bc%9a%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="28 最简单的概率图：朴素贝叶斯.md.html">28 最简单的概率图：朴素贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/29%20%e6%9c%89%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="29 有向图模型：贝叶斯网络.md.html">29 有向图模型：贝叶斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/30%20%e6%97%a0%e5%90%91%e5%9b%be%e6%a8%a1%e5%9e%8b%ef%bc%9a%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%9a%8f%e6%9c%ba%e5%9c%ba.md.html" id="30 无向图模型：马尔可夫随机场.md.html">30 无向图模型：马尔可夫随机场.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/31%20%e5%bb%ba%e6%a8%a1%e8%bf%9e%e7%bb%ad%e5%88%86%e5%b8%83%ef%bc%9a%e9%ab%98%e6%96%af%e7%bd%91%e7%bb%9c.md.html" id="31 建模连续分布：高斯网络.md.html">31 建模连续分布：高斯网络.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/32%20%e4%bb%8e%e6%9c%89%e9%99%90%e5%88%b0%e6%97%a0%e9%99%90%ef%bc%9a%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b.md.html" id="32 从有限到无限：高斯过程.md.html">32 从有限到无限：高斯过程.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/33%20%e5%ba%8f%e5%88%97%e5%8c%96%e5%bb%ba%e6%a8%a1%ef%bc%9a%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b.md.html" id="33 序列化建模：隐马尔可夫模型.md.html">33 序列化建模：隐马尔可夫模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/34%20%e8%bf%9e%e7%bb%ad%e5%ba%8f%e5%88%97%e5%8c%96%e6%a8%a1%e5%9e%8b%ef%bc%9a%e7%ba%bf%e6%80%a7%e5%8a%a8%e6%80%81%e7%b3%bb%e7%bb%9f.md.html" id="34 连续序列化模型：线性动态系统.md.html">34 连续序列化模型：线性动态系统.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/35%20%e7%b2%be%e7%a1%ae%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e9%87%8f%e6%b6%88%e9%99%a4%e5%8f%8a%e5%85%b6%e6%8b%93%e5%b1%95.md.html" id="35 精确推断：变量消除及其拓展.md.html">35 精确推断：变量消除及其拓展.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/36%20%e7%a1%ae%e5%ae%9a%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9a%e5%8f%98%e5%88%86%e8%b4%9d%e5%8f%b6%e6%96%af.md.html" id="36 确定近似推断：变分贝叶斯.md.html">36 确定近似推断：变分贝叶斯.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/37%20%e9%9a%8f%e6%9c%ba%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%ef%bc%9aMCMC.md.html" id="37 随机近似推断：MCMC.md.html">37 随机近似推断：MCMC.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/38%20%e5%ae%8c%e5%a4%87%e6%95%b0%e6%8d%ae%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9a%e6%9c%89%e5%90%91%e5%9b%be%e4%b8%8e%e6%97%a0%e5%90%91%e5%9b%be.md.html" id="38 完备数据下的参数学习：有向图与无向图.md.html">38 完备数据下的参数学习：有向图与无向图.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/39%20%e9%9a%90%e5%8f%98%e9%87%8f%e4%b8%8b%e7%9a%84%e5%8f%82%e6%95%b0%e5%ad%a6%e4%b9%a0%ef%bc%9aEM%e6%96%b9%e6%b3%95%e4%b8%8e%e6%b7%b7%e5%90%88%e6%a8%a1%e5%9e%8b.md.html" id="39 隐变量下的参数学习：EM方法与混合模型.md.html">39 隐变量下的参数学习：EM方法与混合模型.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/40%20%e7%bb%93%e6%9e%84%e5%ad%a6%e4%b9%a0%ef%bc%9a%e5%9f%ba%e4%ba%8e%e7%ba%a6%e6%9d%9f%e4%b8%8e%e5%9f%ba%e4%ba%8e%e8%af%84%e5%88%86.md.html" id="40 结构学习：基于约束与基于评分.md.html">40 结构学习：基于约束与基于评分.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e5%a6%82%e4%bd%95%e6%88%90%e4%b8%ba%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%b7%a5%e7%a8%8b%e5%b8%88%ef%bc%9f.md.html" id="如何成为机器学习工程师？.md.html">如何成为机器学习工程师？.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 机器学习的模型体系.md.html">总结课 机器学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e6%80%bb%e7%bb%93%e8%af%be%20%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a8%a1%e5%9e%8b%e4%bd%93%e7%b3%bb.md.html" id="总结课 贝叶斯学习的模型体系.md.html">总结课 贝叶斯学习的模型体系.md.html</a>
</li>
<li>
<a class="menu-item" href="/%e4%b8%93%e6%a0%8f/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a040%e8%ae%b2/%e7%bb%93%e8%af%be%20%e7%bb%88%e6%9c%89%e4%b8%80%e5%a4%a9%ef%bc%8c%e4%bd%a0%e5%b0%86%e4%b8%ba%e4%bb%8a%e5%a4%a9%e7%9a%84%e4%bb%98%e5%87%ba%e9%aa%84%e5%82%b2.md.html" id="结课 终有一天，你将为今天的付出骄傲.md.html">结课 终有一天，你将为今天的付出骄傲.md.html</a>
</li>
<li><a href="/assets/捐赠.md.html">捐赠</a></li>
</ul>
</div>
</div>
<div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseleave="remove_inner()" onmouseover="add_inner()">
<div class="sidebar-toggle-inner"></div>
</div>
<div class="off-canvas-content">
<div class="columns">
<div class="column col-12 col-lg-12">
<div class="book-navbar">
<header class="navbar">
<section class="navbar-section">
<a onclick="open_sidebar()">
<i class="icon icon-menu"></i>
</a>
</section>
</header>
</div>
<div class="book-content" style="max-width: 960px; margin: 0 auto;
    overflow-x: auto;
    overflow-y: hidden;">
<div class="book-post">
<div align="center">因收到Google相关通知，网站将会择期关闭。<a href="https://lumendatabase.org/notices/44265620" target="_blank">相关通知内容</a><hr/></div>
<p align="center" id="tip"></p>
<h1 class="title" data-id="37 随机近似推断：MCMC" id="title">37 随机近似推断：MCMC</h1>
<div><p>本质上说，确定性近似是遵循着一定的原则，使用一个分布来近似另一个分布，近似结果取决于确定的规则。可是在很多预测任务中，完整的后验分布并不是必需的，我们关注的对象只是某个因变量在后验分布下的期望，或者具有最大后验概率的那个取值。这时再使用确定性近似来计算预测结果，尤其是连续函数在连续分布下的预测结果又是个在计算上颇为棘手的问题。</p>
<p>有些时候，即使目标分布的形式是已知的，对它的求解也存在着困难。就拿常见的Beta分布来说，其概率密度可以表示为<span class="math inline">\(p(x) = Cx^{\\alpha - 1}(1 - x)^{\\beta - 1}\)</span>，其中常数<span class="math inline">\(\\alpha, \\beta\)</span>都是分布参数，常数<span class="math inline">\(C\)</span>是归一化因子。可问题在于如果不能计算出这个复杂的参数<span class="math inline">\(C\)</span>，即使能够确定分布的形状，也没法对分布进行直接的采样。这种情况下也要借助随机性近似。</p>
<p>既然求解解析解既复杂繁冗又无甚必要，那就不妨用统计学的核心概念——抽样来解决问题。<strong>用样本分布来代替难以求解的后验分布，这就是随机性近似的思想</strong>。</p>
<p>随机性近似（stochastic approximation）属于<strong>数值近似</strong>（numerical approximation）的范畴，它对数据的生成机制进行建模，通过模型生成符合真实分布的抽样结果，再利用这些抽样结果表示未知的概率分布。</p>
<p><strong>随机性近似的典型方法是马尔可夫链蒙特卡洛方法</strong>（Markov Chain Monte Carlo method），简称 <strong>MCMC</strong>。其作用是在概率空间中构造合适的马尔科夫链，再应用蒙特卡洛方法进行随机采样来拟合目标的分布。</p>
<p><strong>MCMC体现的是真正的概率密度的思想</strong>，它虽然不能计算分布的表达式，却可以将概率等比例地放大。频率意义下的概率就是数据出现的频度，归一化的作用只是让它变成公理化的概率，而不会对频率解释产生任何影响。</p>
<p>MCMC的出发点就在于消除掉那个不影响分布趋势却又没它不行的归一化常数<span class="math inline">\(C\)</span>对概率求解的影响，通过对简单分布（比如均匀分布）进行抽样来拟合出更加复杂，甚至于压根儿不存在解析式的分布形式。</p>
<p>虽然都可以缩写成MC，但马尔可夫链和蒙特卡洛方法却是两个完全不同的概念。</p>
<p>蒙特卡洛方法诞生于曼哈顿计划中，其缔造者是数学家斯坦尼斯拉夫·乌拉姆（Stanislaw Ulam）和不世出的天才约翰·冯诺伊曼（John von Neumann）。蒙特卡洛本身是袖珍王国摩纳哥的一块国土，以其大名鼎鼎的蒙特卡洛赌场闻名于世，这样的名字或多或少地说明了这个方法和作为概率论不竭灵感源泉的赌博娱乐之间的深厚渊源。</p>
<p>这个号称20世纪最伟大的算法其实不难理解。通俗地说，它就是<strong>通过多次独立重复的随机实验来模拟确定性的概率分布，或者求解难以计算的求和问题，其精确性由大数定律所保证</strong>。</p>
<p>蒙特卡洛方法最广为人知的应用可能就是对圆周率<span class="math inline">\(\\pi\)</span>的估算：在一个单位面积的正方形里随机且均匀地抛洒若干个点，然后统计这些点中和某个选取出的顶点之间距离小于1的点的数目。</p>
<p>如果将这个选出来的参考顶点视为圆心，那么和它的距离小于1的这些点就都在四分之一圆内，四分之一圆内的点数和所有点数的比例就是<span class="math inline">\(\\pi / 4\)</span>的估计值。当随机生成的点数达到30000时，<span class="math inline">\(\\pi\)</span>的估计误差可以下降到0.07%以下。</p>
<p><img alt="" src="assets/eb0945aa2185df958f4568e58300e77a.gif"/></p>
<p>用蒙特卡洛法估计<span class="math inline">\(\\pi\)</span>值（图片来自维基百科）</p>
<p>使用蒙特卡洛方法估计未知的目标分布<span class="math inline">\(p(x)\)</span>时，可以先引入另一个概率分布<span class="math inline">\(q(x)\)</span>作为参考分布，这个参考分布被称为<strong>建议分布</strong>（proposal distribution），具有简单的形式和易于采样的特性。与建议分布配套的还有个常数<span class="math inline">\(M\)</span>，两者共同满足<span class="math inline">\(Mq(x) \\ge {\\tilde p}(x)\)</span>，这里的<span class="math inline">\({\\tilde p}(x)\)</span>是未归一化的概率，是目标分布<span class="math inline">\(p(x)\)</span>与另一个常数<span class="math inline">\(Z\)</span>的乘积。</p>
<p>如果将上面的两个准概率分布画在同一个坐标系里，<span class="math inline">\(Mq(x)\)</span>对应的曲线会将<span class="math inline">\({\\tilde p}(x) = Zp(x)\)</span>对应的曲线完全包住，两者之间会存在一段间隔。在执行采样时，首先按照概率分布<span class="math inline">\(q(x)\)</span>生成一个随机数<span class="math inline">\(x_0\)</span>，接着在<span class="math inline">\(\[0, Mq(x_0)\]\)</span>的区间上通过均匀采样采出来一个新数<span class="math inline">\(u_0\)</span>。如果得到的<span class="math inline">\(u_0\)</span>大于<span class="math inline">\({\\tilde p}(x_0)\)</span>，那它就落在两条曲线之间的区域，这样的样本会被直接抛弃；如果<span class="math inline">\(u_0\)</span>小于<span class="math inline">\({\\tilde p}(x_0)\)</span>，那它就落在<span class="math inline">\({\\tilde p}(x)\)</span>曲线的下方，这样的样本才会保留。</p>
<p><img alt="" src="assets/34ccdb2c4ff5b1cd36db6c53cee6d3bd.png"/></p>
<p>拒绝采样示意图</p>
<p>图片来自《机器学习》（Machine Learning）第50卷第1期5-43，《用于机器学习的MCMC介绍》（An introduction to MCMC for machine learning）。</p>
<p>由于需要根据样本的特性决定接受或是拒绝，因而以上的采样机制被称为<strong>拒绝采样</strong>（rejection sampling）。可以证明，拒绝采样等效于对目标分布<span class="math inline">\(p(x)\)</span>进行多次独立的采样。一般说来，即使对系数<span class="math inline">\(Z\)</span>进行优化处理，拒绝采样也会有较高的拒绝率，其运算效率通常较低。</p>
<p>蒙特卡洛方法只是随机采样的过程，而要确保采出来的样本服从我们想要的分布，需要借助第一个MC：<strong>马尔可夫链</strong>。马尔可夫链假定每一时刻的状态只依赖于前一时刻的状态，每一个状态又会以一定的概率变化为另一个状态，状态之间所有的转化概率共同构成了这个马尔可夫链的状态转移矩阵（transition matrix）。</p>
<p><strong>转移矩阵可以将任意的初始概率分布变成马尔可夫链的稳态分布</strong>（equilibrium distribution）。稳态分布由转移矩阵决定，而与初始的概率分布无关，不管每个状态的初始概率如何，经过若干轮次的转换之后，都可以得到符合稳态分布的样本。这意味着如果能够计算出某个稳态分布所对应的马尔科夫链的状态转移矩阵，服从这个稳态分布的数据样本就唾手可得。</p>
<p>引入了马尔可夫性后，MCMC最原始的实现——<strong>Metropolis算法</strong>便呼之欲出。Metropolis算法可以看成是结合了马尔可夫链的拒绝采样，它将原始的数据点初始化为<span class="math inline">\(x^{0}\)</span>，将转移概率初始化为<span class="math inline">\(q(x | x^{0})\)</span>。需要注意的是，Metropolis算法中的转移概率必须具备对称的特性，也就是<span class="math inline">\(q(x | y) = q(y | x)\)</span>对任意的<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>都成立。</p>
<p>在每一轮数据的生成中，Metropolis算法会根据上一轮的结果<span class="math inline">\(x^{t}\)</span>和以建议分布形式出现的转移概率<span class="math inline">\(q(x | x^{t})\)</span>生成<span class="math inline">\(x’\)</span>，这个新生成的样本将以<span class="math inline">\(\\alpha = {\\tilde p}(x’) / {\\tilde p}(x^{(t)})\)</span>的概率被接受。在实现中，接受的策略体现为生成一个在[0, 1]上均匀分布的随机数<span class="math inline">\(u\)</span>，如果<span class="math inline">\(u &lt; \\alpha\)</span>就接收新样本，反之则继续使用上一轮次的旧样本。如果新样本的出现引起了<span class="math inline">\({\\tilde p}(\\cdot)\)</span>的增加，也就是<span class="math inline">\(\\alpha &gt; 1\)</span>的话，这样的新样本就是必然被保留的。</p>
<p>直观理解，在模拟分布时，Metropolis就像一只在山头上游弋，标记自己领地的老虎。它会在概率密度大，也就是数据出现频度高的位置多转几圈，多留下自己的气味；而在概率密度小，数据出现频度低的位置，示意性地巡逻一下，留一点味道就可以了。数据的概率密度正是通过气味的浓度，也就是采样的样本出现的次数所定义的。</p>
<p>但这样的行为又带来了一个问题，那就是Metropolis在生成新样本时更倾向于接收那些来自分布集中区域的样本。如果新样本导致准概率密度的增加，那它就100%会被接受。可如果导致准概率下降，新样本就存在被丢弃的可能，下降的程度越大，被接受的概率就越低，这将会导致生成的样本更容易抱团在一起。如果将Metropolis算法中对称的建议分布设定为以上一轮的结果为中心的高斯分布，生成的序列就会变成围着一个小区域转圈圈的随机游动（random walk）。</p>
<p>将原始Metropolis算法的中建议分布的对称特性去掉，得到的就是广泛应用的<strong>Metropolis-Hastings算法</strong>，简称<strong>MH算法</strong>。</p>
<p>马尔可夫链的特性决定了根据给定的状态矩阵确定对应的稳态分布是小菜一碟，可根据给定的稳态分布找到对应状态矩阵的难度却有如大海捞针。为了简化寻找转移矩阵的难度，MH算法利用了稳态分布的一个充分非必要条件，那就是<strong>细致平稳性</strong>（detailed balance）。细致平稳性的数学表达式可以写成</p>
<p><span class="math display">\[ \\pi(i) {\\bf Q}_{ij} = \\pi(j) {\\bf Q}_{ji} \]</span></p><p>其中<span class="math inline">\(\\pi\)</span>是马尔可夫链的稳态分布，<span class="math inline">\(\\bf Q\)</span>是马尔可夫链的转移矩阵。这个式子的含义在于从状态<span class="math inline">\(i\)</span>转移到状态<span class="math inline">\(j\)</span>的概率质量恰好等于从<span class="math inline">\(j\)</span>转移回<span class="math inline">\(i\)</span>的概率质量，转入和转出之间存在动态平衡，分布<span class="math inline">\(\\pi\)</span>就是稳态分布。但在具体问题中，任意选择的目标分布<span class="math inline">\(p(x)\)</span>和起到转移矩阵作用的建议分布<span class="math inline">\(q(x)\)</span>很难满足细致平稳性，这时就需要对它们做一些人为的修正，修正方式是引入参数<span class="math inline">\(\\alpha\)</span>，令它满足</p>
<p><span class="math display">\[ p(i)Q(i, j)\\alpha(i, j) = p(j)Q(j, i)\\alpha(j, i) \]</span></p><p>不难看出，参数的引入使转移矩阵被修正为<span class="math inline">\({\\bf Q}(\\cdot)\\alpha(\\cdot)\)</span>，这可以避免Metropolis算法对小概率样本的一刀切。在MH算法中，参数<span class="math inline">\(\\alpha\)</span>就是接受率，可以理解为执行这次从<span class="math inline">\(i\)</span>到<span class="math inline">\(j\)</span>的转移的概率。要让接受率满足上面的条件，最简单的方式是设定两者之中较大的一个为1，再利用等式关系计算出另外一个，这样生成的样本分布<span class="math inline">\(p(z)\)</span>就是马尔可夫链的稳态分布。</p>
<p>MH算法的一个特例是针对高维分布的<strong>吉布斯采样</strong>（Gibbs sampling）。在一个<span class="math inline">\(N\)</span>元分布中计算每个变量关于其他所有变量的条件分布，可以得到<span class="math inline">\(N\)</span>个一元条件分布。吉布斯采样就是对这些条件分布进行采样：在给定初始值后，吉布斯采样按照每个一元条件分布依次产生新的样本并全部接受，作为下一轮更新的基础。</p>
<p>追根溯源，吉布斯采样来源于对吉布斯随机场（Gibbs random field）的研究，它相当于将一个高维的马尔可夫链庖丁解牛，拆解成多个一维的马尔可夫链，高维马尔可夫链整体的状态转移也相应地被拆解成不同一维链轮流的状态转移。这样的拆解并不会影响到细致平稳条件，因而得到的分布依然是目标的稳态分布。另外，<strong>吉布斯采样并不会拒绝产生的样本，这使它和MH算法相比具有效率上的优势，从而成为应用最广泛的MCMC算法</strong>。</p>
<p>利用PyMC库可以实现MCMC采样。出于便于对比的考虑，本讲以估计硬币正反面的概率为例。硬币出现正面的概率<span class="math inline">\(p\)</span>可以用二项分布建模，其先验和后验则用共轭的贝塔分布建模，因而可以给出后验概率精确的解析结果。利用投掷50次硬币出现20次正面的数据，就可以用Metropolis算法来估计二项分布的参数<span class="math inline">\(p\)</span>了。结果表明，MCMC的结果和解析结果基本吻合。</p>
<p>今天我和你分享了MCMC方法的基本原理，以及MH算法和吉布斯采样等具体的实现方式，包含以下四个要点：</p>
<ul>
<li><p>MCMC是基于随机性近似的推断方法；</p></li>
<li><p>MCMC利用基于蒙特卡洛方法的随机采样将任意的初始分布转化为马尔可夫链的稳态分布；</p></li>
<li><p>MCMC的关键问题是找到和目标稳态分布匹配的转移矩阵；</p></li>
<li><p>MCMC的典型方法包括一维的MH算法和多维的吉布斯采样。</p></li>
</ul>
<p>在MCMC中，转移概率或者建议分布<span class="math inline">\(q(x, y)\)</span>的选择是关键因素，其设计的优劣会直接影响到算法的性能。</p>
<p>你可以查阅相关文献，了解转移概率常见的设计思想，并在这里留下你的见解。</p>
<p><img alt="" src="assets/b6a23dde9947f887513575d2a35c4795.jpg"/></p>
</div>
</div>
<div>
<div id="prePage" style="float: left">
</div>
<div id="nextPage" style="float: right">
</div>
</div>
</div>
</div>
</div>
<div class="copyright">
<hr/>
<p>© 2019 - 2023 <a href="/cdn-cgi/l/email-protection#c8a4a4a4f1fcf9f9f8ff88afa5a9a1a4e6aba7a5" target="_blank">Liangliang Lee</a>.
                    Powered by <a href="https://github.com/gin-gonic/gin" target="_blank">gin</a> and <a href="https://github.com/kaiiiz/hexo-theme-book" target="_blank">hexo-theme-book</a>.</p>
</div>
</div>
<a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9359df88bdf33afa',t:'MTc0NTU0MjUwOC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NPSEEVD756"></script>
<script src="/static/index.js"></script>
</head></html>